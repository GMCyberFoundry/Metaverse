
\section{History and market need}

The word metaverse was coined by the author Neal Stephenson in his 1992 novel Snowcrash. It started popping up soon after in \href{https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/}{news articles} and research papers \cite{mclellan1993avatars}.\\
There were clear precursors such as \href{https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/}{VRML in the 1990's} which laid much of the groundwork for 3D content over networked computers. The author used to create commercial 3D scenes on Silicon Graphics systems back in the late 90s.\\
It might seem that there would be a clear path from there to now, in terms of a metaverse increasingly meaning connected social virtual spaces, but this has not happened. Instead interest in metaverse as a concept waned, MMORG (described later) filled in the utility, and then recently an entirely new definition emerged. The concept of the Metaverse is extremely plastic at this time (Figure \ref{fig:muskWeb3}). What this book will aim to build toward is an understanding of metaverse which really means a useful social mixed reality layer, that allows low friction communication and economic activity, within groups, at a global scale.\\
This section will attempt to frame the context, and explain the increasingly polarised options looking forward. It's very important to identify the primitives of the product we would like to see here, so this chapter is far more a review of academic literature in the field.\\

\begin{figure}
  \centering
    \includegraphics[width=\linewidth]{muskWeb3}
  \caption{Elon Musk agrees with this on Twitter}
  \label{fig:muskWeb3}
\end{figure}

\section{Business communication tools}
\subsection{Traditional telepresence}
Video-conferencing has become more popular as technology improves, and with increasing demands for real-time communication across greater distances. The full effects of video-conferencing on human communication are still being explored. Video-conferencing is presumed to be a somewhat richer form of communication than email and telephone, but not quite as informative as face-to-face communication. In this section we look at the influence of eye contact on communication and how video-conferencing mediates both verbal and non-verbal interactions. Facilitation of eye contact is a challenge that must be addressed so that video-conferencing can approach the rich interactions of face-to-face communication. This is an even bugger problem in the emerging metaverse systems, so it's important that we examine the history and trajectory.\\
Connection of multiple users is now well supported through technologies such as telephony \& webcams, with Zoom and \href{https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast}{Mircosoft Teams} alone supporting hundreds of millions of chats a day. This is a 20x increase on market leader Skype's 2013 figure of \href{https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast}{280 million} connections per month. Such technologies extend traditional telephony to provide important multi sensory cues (non-verbal communication \cite{Argyle1976, Wolff2008}).  However, these technologies demonstrate shortfalls compared to a live face-to-face meeting, which is generally agreed to be optimal for human-human interaction \cite{Wolff2008, 2002}.\\
In an experiment at CTrip, Home working led to a 13\% performance increase, of which about 9\% was from working more minutes per shift (fewer breaks and sick-days) and 4\% from more calls per minute (attributed to a quieter working environment). Home workers also reported improved work satisfaction and experienced less turnover, but their promotion rate conditional on performance fell. Due to the success of the experiment, CTrip rolled-out the option to WFH to the whole firm and allowed the experimental employees to re-select between the home or office. Interestingly, over half of them switched, which led to the gains from WFH almost doubling to 22\%. This highlights the benefits of learning and selection effects when adopting modern management practices like WFH.\\
Enterprise collaboration systems provide rich document management, sharing, and collaboration functionality across an organisation. The enterprise ECS system may integrate collaborative video \cite{prakash2020characteristic}. This is for instance the case with Microsoft Teams / Sharepoint. This integration of ECS should be considered when thinking about social VR systems which wish to support business, value, and trust.\\
\subsubsection{Pandemic drives adoption}
The ongoing global COVID-19 pandemic is changing how people work, toward a new global 'normal'. Some ways of working are overdue transformation, and will be naturally disrupted. In the UK at least it seems that there may be `real appetite' to shift away from old practises \cite{skychange}. This upheaval will inevitably present both challenges and opportunities.\\
Highly technical workforces, especially, can operate from anywhere \cite{globalworkplace}. Bloom at al found that the 10\% of home workers surveyed in 2013 enjoyed increased performance and satisfaction, and reduced staff turnover \cite{Bloom2015-rg}. Little seems to have changed since then, until the recent pandemic forced the hand of global business. If only a small percentage of companies allow the option of remote working, then they gain a structural advantage, enjoying benefits of reduced travel, lower workplace infection risk across all disease, and global agility for the personnel. Building and estate costs will certainly be reduced. More diversity may be possible. Issues such as sexual harassment and bullying may be reduced.  With reduced overheads product quality may increase. If customers are happier with their services, then over time this `push' may mean an enormous shift away from centralised working practises toward distributed working. \\
Technologies which can support this working style are (surprisingly) still in their infancy. The rush to `Zoom', a previously relatively unknown and insecure \cite{aiken2020zooming} web meeting product, shows how naive businesses were in this space. \\
While the research community and business are learning how to adapt working practises to web based telepresence \cite{oeppen2020human}, there remains little technology support for ad-hoc serendipitous meetings between small groups. It's possible that Metaverse applications can help to fill this gap, by gamification of social spaces.\\

\subsubsection{Mona Lisa Type Effects}
Almost all traditional group video meeting tools suffer from the so-called Mona Lisa effect describes the phenomenon where the apparent gaze of a portrait or 2 dimensional image always appears to look at the observer regardless of the observer's position \cite{Vishwanath2005, Anstis1969, Wollaston1824}. This situation manifests when the painted or imaged subject is looking into the camera or at the eyes of the painter \cite{Loomis2008, Fullwood2006a}.\\
Single user-to-user systems based around bidirectional video implicitly align the user's gaze by constraining the camera to roughly the same location as the display. When viewed away from this ideal axis, it creates the feeling of being looked at regardless of where this observer is \cite{AlMoubayed2012, Vishwanath2005, Anstis1969, Wollaston1824}, or the ``collapsed view effect'' \cite{Nguyen2005} where perception of gaze transmitted from a 2 dimensional image or video is dependent on the incidence of originating gaze to the transmission medium. \\
Multiple individuals using one such channel can feel as if they are being looked at simultaneously, leading to a breakdown in the normal non-verbal communication which mediates turn passing \cite{Vertegaal2002}. This was mentioned briefly earlier when describing the Cisco Telepresence suites which often have a single central camera.\\
It seems that the effect is truely limited to 2D surfaces. A 3D projection surface (a physical model of a human) designed to address this problem completely removed the Mona Lisa effect \cite{Moubayed2012}.\\
\subsection{Other Systems to Support Business}                  
There have been many attempts to support group working and rich data sharing between dispersed groups in a business setting. So called 'smart spaces' allow interaction with different displays for different activities and add in some ability to communicate with remote or even mobile collaborators on shared documents \cite{Bardram2012}, with additional challenges for multi-disciplinary groups who are perhaps less familiar with one or more of the technology barriers involved \cite{Adamczyk2007}.\\
Early systems like clearboard \cite{Ishii1993} demonstrated the potential for smart whiteboards with a webcam component for peer to peer collaborative working. Indeed it is possible to support this modality with Skype and a smartboard system (and up to deployments such as Accessgrid). They remain relatively unpopular however.\\

\section{What's important for human communication}
\subsection{Extra verbal content}
Additional to the semantic content of verbal communication there is a rich layer of meaning in pauses, gaps, interruptions, and overlaps \cite{Heldner2010} which help to mediate who is speaking and who is listening in multi-party conversation. This mediation of turn passing, to facilitate flow, is by no means a given and is highly dependent on context and other factors \cite{Kleinke1986a}. This stuff is a huge challenge in conventional telepresence, and even more so in third person metaverse applications. \\ 
This extra-verbal content \cite{ting2012understanding} extends into physical cues, so-called `nonverbal' cues, and there are utterances which link the verbal and non-verbal \cite{Otsuka2005}. This will be discussed later, but to an extent, it is impossible to discuss verbal communication without regard to the implicit support which exists around the words themselves. \\
In the context of all technology-mediated conversation the extra-verbal is easily compromised if technology used to support communication over a distance does not convey the information, or conveys it badly. This can introduce additional complexity \cite{Otsuka2005}. These support structures are pretty much lacking in metaverse XR systems. The goal then here perhaps is to examine the state of the art, and remove as many of the known barriers as possible. Such a process might better support trust, which might better support the kind of economic and activity we seek to engineer.\\
\subsection{Audio}
Even in simple voice telephony, system latency, the inherent delays added by the communication technology, can allow slips or a complete breakdown of 'flow' \cite{Katagiri2007}.\\
It seems that transmission of verbal / audio is the most critical element for interpersonal communication since the most essential meaning is encoded semantically. There is a debate about ratios of how much information is conveyed through the various human channels \cite{loomis2012sensory}, but it is reasonable to infer from its ubiquity that support for audio is essential for meaningful communication over a distance. We have seen that it must be timely, to prevent a breakdown of framing, and preferably have sufficient fidelity to convey sub-vocal utterances. For business users of metaverse tools this implies that much better microphones are important, and spatial audio cues within the environment, keyed to the head position. \\
\subsection{Nonverbal}
We have already seen that verbal exchanges take place in a wider context of sub vocal and physical cues. In addition, the spatial relationship between the parties, their focus of attention, their gestures and actions, and the wider context of their environment all play a part in communication \cite{Goodwin2000a}. These are summarised well by Gillies and Slater \cite{Gillies2005} in their paper on virtual agents. In support of this an immersive virtual gathering should have as wide a field of view as possible. This is principally are hardware constraint, though an over wide FOV can be compressed into a smaller hardware display with ``some'' user discomfort.\\
\subsubsection{Gaze}
Of particular importance is judgement of eye gaze which is fast, accurate and automatic, operating at multiple levels of cognition through multiple cues \cite{Argyle,argyle1976gaze,Argyle1965,Argyle1976,Argyle1969, Kendon1967,Monk2002}.\\
Gaze in particular aids with smooth turn passing \cite{Hedge1978} \cite{Novick1996} and lack of support for eye gaze has been found to decrease the efficiency of turn passing by 25\% \cite{Vertegaal00effectsof}.\\
There are clear patterns to eye gaze in groups, with the person talking, or being talked to, probably also being looked at \cite{Vertegaal2001} \cite{Langton2000}. To facilitate this groups will tend to position themselves to maximally enable observation of the gaze of the other parties \cite{Kendon1967}. This intersects with proxemics which will be discussed shortly.  In general people look most when they are listening, with short glances of 3-10 seconds \cite{Argyle1965}. 
Colburn et al. suggest that gaze direction and the perception of the gaze of others directly impacts social cognition \cite{Colburn2000a} and this has been supported in a follow up study \cite{Macrae2002}.\\
The importance of gaze is clearly so significant in evolutionary terms that human acuity for eye direction is considered high at ~30 sec arc \cite{Symons2004} with straight binocular gaze judged more accurately than straight monocular gaze \cite{Kluttz2009}, when using stereo vision. \\
Regarding the judgement of the gaze of others Symons et al. suggesting that ``people are remarkably sensitive to shifts in a person's eye gaze'' in triadic conversation \cite{Symons2004}. Wilson et al. found that subjects can ``discriminate gaze focused on adjacent faces up to [3.5m]'' \cite{Wilson2000}\\
Schrammel et al. investigated to what extent embodied agents can elicit the same responses in eye gaze detection \cite{Schrammel2007}.\\
This perception of the gaze of others operates at a low level and is automatic. Langton et al. cite research stating that the gaze of others is ``able to trigger reflexive shifts of an observer's visual attention'' and further discuss the deep biological underpinnings of gaze processing \cite{doi:10.1080/713755908}. \\          
When discussing technology-mediated systems Vertegaal \& Ding suggested that understanding the effects of gaze on triadic conversation is ``crucial for the design of teleconferencing systems and collaborative virtual environments'' \cite{Vertegaal2002}, and further found correlation between the amount of gaze, and amount of speech. Vertegaal \& Slagter suggest that ``gaze function(s) as an indicator of conversational attention in multiparty conversations'' \cite{Vertegaal2001}. \\        
Vertegaal et al. found that task performance was 46\% better when gaze was synchronised in their telepresence scenario. As they point out, gaze synchronisation (temporal and spatial) is `commendable' in all such group situations, but the precise utility will depend upon the task \cite{Vertegaal2002}.\\
There has been some success in the automatic detection of the focus of attention of participants in multi party meetings \cite{Stiefelhagen2001, Stiefelhagen2002}.  More recently, advanced eye tracking technologies allow the recording and replaying of accurate eye gaze information \cite{Steptoe2009} alongside information about pupil dilation toward determination of honesty and social presence \cite{Steptoe2010a}.\\               
In summary, gaze awareness does not just mediate verbal communication but rather is a complex channel of communication in its own right. Importantly, gaze has a controlling impact on those who are involved in the communication at any one time, including and excluding even beyond the current participants. It seems of importance for our metaverse use case that gaze be possible, if not necessarily demanded. \\
\subsubsection{Mutual Gaze}
Aygyle and Cook established a great deal of science around gaze and mutual gaze, with their seminal book of the same title \cite{argyle1976gaze}, additionally detailing confounding factors around limitations and inaccuracies in observance of gaze and how this varies with distance \cite{Argyle1969} \cite{Argyle} \cite{Cook1977}.\\
Mutual gaze is considered to be the most sophisticated form of gaze awareness with significant impact on dyadic conversation especially \cite{Cook1977, Kleinke1986a, Fagel2010}. The effects seem more profound than just helping to mediate flow and attention, with mutual eye gaze aiding in memory recall and the formation of impressions \cite{Bohannon2013}.\\
While reconnection of mutual eye gaze through a technology boundary does not seem completely necessary it is certainly important, with impact on subtle elements of one to one communication, and therefore discrimination of eye gaze direction should be bi-directional if possible, and if possible have sufficient accuracy to judge direct eye contact. In their review Bohannon et al. said that the issue of rejoining eye contact must be addressed in order to fully realise the richness of simulating face to face encounters \cite{Bohannon2013}.\\
Mutual gaze is a challenging affordance as bi-directional connection of gaze is not a trivial problem. All users would need to be immersed and using eye trackers properly. There is no technical impediment here, but rather one of adoption curves.
\subsubsection{Head Orientation}
Orientation of the head (judged by the breaking of bilateral symmetry and alightment of nose) is a key factor when judging attention. Perception of head orientation can be judged to within a couple of degrees \cite{Wilson2000}.\\
It has been established that head gaze can be detected all the way out to the extremis of peripheral vision, with accurate eye gaze assessment only achievable in central vision \cite{Loomis2008}. Features of illumination can alter the apparent orientation of the head \cite{Troje1998}; as usual the wider context is in play.\\
Head motion over head orientation is a more nuanced proposition and can be considered a micro gesture \cite{Boker2011}.\\
Regarding video mediated teleconferencing;  Vertegaal  et al. \cite{Vertegaal00effectsof} note that  the ``larger the distance of head to screen, or the smaller the projected images, the more head movement of users is tolerable without impairing the conveyance of gaze at the eyes''. While users of video conferencing equipment tend to position themselves correctly in front of the camera, movement speed of the head within the frame during use is typically in the region 2-3 meters per second. Framerate of the camera should support this \cite{Bocker1996}.\\
                    It is possible that 3D displays are better suited to perception of head gaze since it is suggested that they are more suitable for ``shape understanding tasks'' \cite{john2001use}\\
                    Bailenson, Baell, and Blascovich found that giving avatars rendered head movements in a shared virtual environment decreased the amount of talking, possibly as the extra channel of head gaze was opened up. They also reported that subjectively, communication was enhanced \cite{Bailenson2002}. \\
                    Clearly head orientation is an important indicator of the direction of attention of members of a group and can be discerned even in peripheral vision. This allows the focus of several parties to be followed simultaneously and is an important affordance to replicate on any multi-party communication system. Head gaze comes for free in immersive headset participation within a metaverse, but how to handle users who do not have a headset?\\
\subsubsection{Combined Head and Eye Gaze}
Rienks et al. found that head orientation alone does not provide a reliable cue for identification of the speaker in a multiparty setting \cite{Rienks2010}. Stiefelhagen \& Zhu found ``that head orientation contributes 68.9\% to the overall gaze direction on average'' \cite{Stiefelhagen:2002:HOG:506443.506634}, though head and eye gaze seem to be judged interdependently \cite{Kluttz2009}. Langton noted that head and eye gaze are ``mutually influential in the analysis of social attention'' \cite{Langton2000a}, and it is clear that transmission of `head gaze' by any mediating system enhances rather than replaces timely detection of subtle cues. Combined head and eye gaze give the best of both worlds and extend the lateral field of view in which attention can be reliably conveyed to others. This implied both head AND eye tracking for participants in the metaverse. There are edge cases here where eye tracking might be present without head tracking, and they should be identified in the system.\\

\subsection{Presence, Co-presence, and Social Presence}
Presence is a heavily cited historic indicator of engagement in virtual reality, though the precise meaning has been interpreted differently by different specialisms \cite{Beck2011, Schuemie2001}. It is generally agreed to be the 'sense of being' in a virtual environment \cite{Slater1999}. Slater extends this to include the ``extent to which the VE becomes dominant". \\
Beck et al. reviewed 108 articles and synthesised an ontology of presence \cite{Beck2011} which at its simplest is as follows:
            \begin{enumerate}
				\item Sentient presence
                    \begin{enumerate}
                     \item Physical interaction
                      \item Mental interaction
                    \end{enumerate}
                   \item Non-sentient
                   \begin{enumerate}
                       \item Physical immersion
                       \item Mental immersion = psychological state
                     \end{enumerate}
            \end{enumerate}
When presence is applied to interaction it may be split into Telepresence, and Co/Social presence  \cite{heeter1992being, Biocca1997}.  Co-presence and/or social presence is the sense of ``being there with another", and describes the automatic responses to complex social cues \cite{doi:10.1080/01449299508914633, fulk1987social, haythornthwaite1995work}.    Social presence (and co-presence) refers in this research context to social presence which is mediated by technology (even extending to text based chat \cite{Gunawardena1997}), and has its foundations in psychological mechanisms which engender mutualism in the `real'. This is analysed in depth by Nowak \cite{Nowak2001}. An examination of telepresence, co-presence and social presence necessarily revisits some of the knowledge already elaborated.\\
        The boundaries between the three are blurred in research with conflicting results presented \cite{Bulu2012}. Biocca et al. attempted to enumerate the different levels and interpretations surrounding these vague words \cite{Biocca2003}, and to distill them into a more robust theory which better lends itself to measurement. They suggest a solid understanding of the surrounding psychological requirements which need support in a mediated setting, and then a scope that is detailed and limited to the mediated situation.\\
 Since `social presence' has been subject to varied definitions \cite{Biocca2003} it is useful here to consider a single definition from the literature which defines it as ``the ability of participants in the community of inquiry to project their personal characteristics into the community, thereby presenting themselves to the other participants as real people.'' \cite{Garrison1999, Beck2011}. Similarly to specifically define co-presence for this research it is taken to be the degree to which participants in a virtual environment are ``accesible, available, and subject to one another" \cite{Biocca2003}. \\
            Social presence has received much attention and there are established questionnaires used in the field for measurement of the levels of perceived social presence yet the definitions here also remain broad, with some confusion about what is being measured \cite{Biocca2003a}.\\            
 Telepresence meanwhile is interaction with a different (usually remote) environment which may or may not be virtual, and may or may not contain a separate social/co-presence component. \\ 
       Even in simple videoconferencing Bondareva and Bouwhuis stated (as part of an experimental design) that the following determinants are important to create social presence \cite{Bondareva2004, jouppi2002mutually}. 
            \begin{enumerate}
            \item    Direct eye contact is preserved
            \item    Wide visual field
            \item    Both remote participants appear life size
            \item    Possibility for participants to see the upper body of the interlocutor
            \item    High quality image and correct colour reproduction
            \item    Audio with high S/N ratio
            \item    Directional sound field
            \item    Minimization of the video and audio signal asynchrony
            \item    Availability of a shared working space.
            \end{enumerate}
			
			     
            Bondareva et al. went on to describe a person to person telepresence system with a semi silvered mirror to reconnect eye gaze, which they claimed increased social presence indicators. Interestingly they chose a checklist of interpersonal interactions which they used against recordings of conversations through the system \cite{Bondareva2004}.  \\
            The idea of social presence as an indicator of the efficacy of the system suggests the use of social presence questionnaires in the evaluation of the system \cite{Biocca2003}.  Subjective questionnaires are however troublesome in measuring effectiveness of virtual agents and embodiments, with even nonsensical questions producing seemingly valid results \cite{Slater2004}. Usoh et al. found that 'the real' produced only marginally higher presence results than the virtual \cite{Usoh2000a}.\\
            Nowak states that ``A satisfactory level of co-presence with another mind can be achieved with conscious awareness that the interaction is mediated" and asserts that while the mediation may influence the degree of co-presence it is not a prohibiting factor \cite{Nowak2001}.\\ 
            Baren and IJsselsteijn \cite{Baren, Harms2004} list 20 useful presence questionnaires in 2004 of which ``Networked Minds" seemed most appropriate for the research.
            Hauber et al. employed the ``Networked Minds" Social Presence questionnaire experimentally and found that while the measure could successfully discriminate between unmediated and mediated triadic conversation, it could not find a difference between 2D and 3D mediated interfaces \cite{Hauber2005, Gunawardena1997}.
            \\In summary, social presence and co-presence are important historic measures of the efficacy of a communication system. Use of the term in literature peaked between 1999 and 2006 according to Google's ngram viewer and has been slowly falling off since. The questionnaire methodology has been challenged in recent research and while more objective measurement may be appropriate, the networked minds questions seem to be able to differentiate real from virtual interactions \cite{Harms2004}.
 
\subsection{Mutual Gaze in Telepresence}
          We have seen that transmission of attention can broadly impact communication in subtle ways, impacting empathy, trust, cognition, and co-working patterns. Mutual gaze (looking into one another's eyes), is currently the high water mark for technology-mediated conversation.\\
          Many attempts have been made to re-unite mutual eye gaze when using tele-conferencing systems. In their 2015 review of approaches Regenbrecht and Langlotz found that none of the methods they examined were completely ideal \cite{Regenbrecht2015}. They found most promise in 2D and 3D interpolation techniques, which will be discussed in detail later, but they opined that such systems were very much ongoing research and lacked sufficient optimisation.\\
          A popular approach uses the so called 'Peppers Ghost' phenomenon \cite{steinmeyer2013science}, where a semi silvered mirror presents an image to the eye of the observer, but allows a camera to view through from behind the angled mirror surface. The earliest example of this is Rosental's two way television system in 1947 \cite{rosenthal1947two}, though Buxton et al. `Reciprocal Video Tunnel' from 1992 is more often cited \cite{buxton1992telepresence}. This optical characteristic isn't supported by RPT technology, and besides requires careful control of light levels either side of the semi-silvered surface.\\  
The early GAZE-2 system (which makes use of Pepper's ghost) is novel in that it uses an eye tracker to select the correct camera from several trained on the remote user. This ensures that the correct returned gaze (within the ability of the system) is returned to the correct user on the other end of the network \cite{Vertegaal2003}.
Mutual gaze capability is later highlighted as an affordance supported or unsupported by key research and commercial systems.                           

\subsubsection{Tabletop and Shared Task}
In early telepresence research Buxton and William argued through examples that ``effective telepresence depends on quality sharing of both person and task space \cite{Buxton1992a}.\\
In their triadic shared visual workspace Tang et al. found difficulty in reading shared text using a `round the table' configuration, a marked preference for working collaboratively on the same side of the table. They also found additional confusion as to the identity of remote participants \cite{Tang2010}.
Tse et al. found that pairs can work well over a shared digital tabletop, successfully overcoming a single user interface to interleave tasks \cite{Tse2007}.\\
Tang et al. demonstrate that collaborators engage and disengage around a group activity through several distinct, recognizable mechanisms with unique characteristics \cite{Tang2006}. They state that tabletop interfaces should offer a variety of tools to facilitate this fluidity.\\
Camblend is a shared workspace which is hybrid physical and digital and which maintains some spatial cues between locations \cite{Norris2013a, Norris2012}. Participants successfully resolved such co-orientation within the system.\\
The t-room system implemented by Luff et al. surrounds co-located participants standing at a shared digital table with life sized body and head video representations of remote collaborators \cite{Luff2011} but found that there were incongruities in the spatial and temporal matching between the collaborators which broke the flow of conversation.
Tuddenham et al. found that co-located collaborators naturally devolved 'territory' of working when sharing a task space, and that this did not happen the same way with a tele-present collaborator \cite{tuddenham2009territorial}. Instead remote collaboration adapted to use a patchwork of ownership of a shared task. It seems obvious to say that task ownership is a function of working space, but it is interesting that the research found no measurable difference in performance when the patchwork coping strategy was employed.\\
The nature of a shared collaborative task and/or interface directly impacts the style of interaction between collaborators. This will have a bearing on the choice of task for experimentation \cite{Jamil2011, Jetter2011a}.

            \subsubsection{Spatially Faithful Group}
                Hauber et al. combined videoconferencing, tabletop, and social presence analysis and tested the addition of 3D. They found a nuanced response when comparing 2D and 3D approaches to spatiality: 3D showed improved presence over 2D (chiefly through gaze support), while 2D demonstrated improved task performance because of task focus \cite{Hauber2006}.\\
                I3DVC reconstructs participants from multiple cameras and places them isotropically (spatially faithful) \cite{Kauff2002, Kauff2002a}. The system uses a large projection screen, a custom table, and carefully defined seating positions. They discussed an ``extended perception space" which used identical equipment in the remote spaces in a tightly coupled collaborative `booth'. It employed head tracking and multi camera reconstruction alongside large screens built into the booth. This system exemplified the physical restrictions which are required to limit the problems of looking into another space through the screen. Towles et al. \cite{Fuchs:2002ww} demonstrated a similar system over a wide area network but achieved only limited resolution and frame rate with the technology of the day. \\ University of Southern California used a technically demanding set-up with 3D face scanning and an autostereoscopic 3D display to generate multiple `face tracked' viewpoints \cite{jones2009headspin}. This had the disadvantage of displaying a disembodied head.\\                
MAJIC is an early comparable system to support small groups with life size spatially correct video, but without multiple viewpoints onto the remote collaborators it was a one to 'some' system rather than 'some' to one. Additionally users were rooted to defined locations \cite{Ichikawa1995, Okada:1994et}.\\
\textbf{Multiview}

In order to reconnect directional cues of all kinds it is necessary for each party in the group to have a spatially correct view of the remote user which is particular for them. This requires a multi-view display, which has applications beyond telepresence but are used extensively in research which attempts to address these issues.\\
Nguyen and Canny demonstrated the `Multiview' system \cite{Nguyen2005}. Multiview is a spatially segmented system, that is, it presents different views to people standing in different locations simultaneously. They found similar task performance in trust tasks to face to face meetings, while a similar approach without spatial segmentation was seen to negatively impact performance.\\
In addition to spatial segmentation of viewpoints it is possible to isolate viewpoints in the time domain. Different tracked users can be presented with their individual view of a virtual scene for a few milliseconds per eye, before another viewpoint is shown to another user. Up to six such viewpoints are supported in the c1x6 system \cite{Kulik2011}
Similarly MM+Space offered 4 Degree-Of-Freedom Kinetic Display to recreate Multiparty Conversation Spaces \cite{Otsuka2013}
            \subsubsection{Robots, Shader Lamp, and Hybrid}
                Virtuality human representation extends beyond simple displays into robotic embodiments (which need not be humanoid \cite{Marti2005}), shape mapped projection dubbed ``shader lamps", and hybridisations of the two.\\ 
				\textbf{Uncanniness}
				
When employing simulation representations of humans it may be the case that there is an element of weirdness to some of these systems, especially those that currently represent a head without a body. Mori has demonstrated The Uncanny Valley \cite{Mori1970} effect in which imperfect representations of humans elicit revulsion in certain observers. This provides a toolkit for inspecting potentially `weird' representations, especially if they are `eerie' and is testable through Mori's GODSPEED questionnaire. \\
                    With an improved analysis of the shape of the likeability curve estimated later showing a more nuanced response from respondents where anthropomorphism of characters demonstrated increased likeability even against a human baseline \cite{Bartneck2007, Bartneck2009a}.\\
                    A mismatch in the human realism of face and voice also produces an Uncanny Valley response \cite{Mitchell2011}.\\
                    However, there is a possibility that Mori's hypothesis may be too simplistic for practical everyday use in CG and robotics research since anthropomorphism can be ascribed to many and interdependent features such as movement and content of interaction \cite{Bartneck2009}.\\
                    Bartneck et al. also performed tests which suggest that the original Uncanny Valley assertions may be incorrect, and that it may be inappropriate to map human responses to human simulacrum to such a simplistic scale. They suggest that the measure has been a convenient `escape route' for researchers \cite{Bartneck2009}. Their suggestion that the measure should not hold back the development of more realistic robots holds less bearing for the main thrust of this telepresence research which seeks to capture issues with imperfect video representation rather than test the validity of an approximation.\\
                    Interestingly Ho et al. performed tests on a variety of facial representations using images. This is far closer to the core investigation. They found that facial performance is a `double edged sword' with realism being important to robotic representations, but there also being a significant Uncanny Valley effect around `eerie, creepy, and strange' which can be avoided by good design \cite{Ho2008}.\\
                    More humanlike representations exhibiting higher realism produce more positive social interactions when subjective measures are used \cite{Yee2007a} but not when objective measures are used. This suggests that questionnaires may be more important when assessing potential uncanniness.\\
                    Ho et al. also identified problems with the original GODSPEED indices used to measure Uncanny Valley effects. They proposed a new set of measures which they found to be generally valid \cite{Ho:2010:RUV:1853385.1853509}, though they admit they were only tested with a single set of stimuli.\\
                    A far more objective method would be to measure user responses to humans, robots, and representations with functional near-infrared spectroscopy and while this has been attempted it is early exploratory research \cite{Strait2014}, an emotional response to `eerie' was discovered.\\
\textbf{Embodiment through robots}

                    Robots which carry a videoconference style screen showing a head can add mobility and this extends the available cues \cite{Adalgeirsson2010, Lee2011b, Tsui2011, Paulos1998, Kristoffersson2013}. Interestingly Desai and Uhlik maintain that the overriding modality should be high quality audio \cite{Desai2011}.\\
                    Tsui et al. asked 96 participants to rate how personal and interactive they found interfaces to be. Interestingly they rated videoconferencing as both more personal and more interactive than telepresence robots, suggesting that there is a problem with the overall representation or embodiment \cite{Tsui2012}.\\
                    Kristoffersson et al. applied the Networked Minds questionnaire to judge presence of a telepresence robot for participants with little or no experience of videoconferencing. Their results were encouraging, though they identified that the acuity of the audio channel needing improvement \cite{Kristoffersson2011}.\\
                    There are a very few lifelike robots which can be used for telepresence, and even these are judged to be uncanny \cite{Sakamoto2007}. This is only an issue for a human likeness since anthropomorphic proxies such as robots and toys perform well \cite{Mori1970}.\\
 \textbf{Physical \& Hybrid embodiment}
 
                    Embodiment through hybridisation of real-time video and physical animatronic mannequins has been investigated as a way to bring the remote person into the space in a more convincing way \cite{Lincoln2009, Lincoln:2010it, Raskar2001a}. \ These include telepresence robots \cite{Lee2011b, Sakamoto2007, Tsui2011}, head in a jar implementations such as SphereAvatar \cite{Oyekoya2012, pan2014comparing, Pan2012a} and BiReality \cite{Jouppi2004}, \ UCL's Gaze Preserving Situated Multi-View Telepresence System \cite{Pan2014a}, or screen on a stick style representations \cite{Kristoffersson2013}.\\  
                    Nagendran et al. present a 3D continuum of these systems into which they suggest all such systems can be rated from artificial to real on the three axes, shape, intelligence, and appearance \cite{Nagendran}.\\
                    Itoh et al. describe a 'face robot' to convey captured human emotion over a distance. It uses an `average face' and actuators to manipulate feature points \cite{Itoh2005}. It seems that this is an outlier method for communication of facial affect but demonstrates that there are many development paths to a more tangible human display.\\ 
\textbf{Shader lamps}

Projection mapping is a computational augmented projection technique where consideration of the relative positions and angles of complex surfaces allows the projection from single or multiple sources to augment the physical shapes onto which they appear. It was first considered by the Disney corporation in 1969 \cite{projectionmappingweb2013} and was given prominence by Raskar and Fuchs with ``office of the future" \cite{Raskar1998} and later by Raskar and other researchers \cite{Low2001, Raskar2001a}. It has since gained considerable commercial popularity in live entertainment \cite{googleStatsProjectionMapping}.\\
                    Shader lamps \cite{Raskar2001a} is the more formal academic designation for projection mapping. It is possible to use the technique alongside reconstruction to project onto a white facial mannequin. Researchers have attempted to use the technology for remote patient diagnostic, projecting onto styrofoam heads  \cite{Rivera-Gutierrez2012}.\\          
                     Bandyopadhyay et al. demonstrated \cite{Bandyopadhyay} that it is possible to track objects and projection map \cite{Dalsgaard2011} onto them in real time. This is beyond the scope of the proposed projection onto furniture since we wish to keep the system as simple as possible, but could be useful for shared tasks in the future work.\\
                    Lincoln et al. employed animatronic avatars which they projected with shader lamps. This combination recreated facial expression and head movement though they were limited in speed and range of control of the remote head \cite{Lincoln:2010it}.\\
                    While shader lamps are an important and useful technology, there are limitations imposed by its use. In particular if a realtime video feed or reconstruction of a subject is used then that scanned subject must either remain still enough to be correctly mapped onto geometry on the remote side (useful for some virtual patients for instance \cite{Benjamin2012}, or else there must be a computational adjustment made for their changing position to make them appear static, or the projection surface must move to match their movement as in Lincoln et al. .
 
        \subsubsection{Immersive Collaborative Virtual Environments}
Simple online virtual environments with an external viewpoint and many avatars interacting online generated much hype in the early days as a potential means to support group meetings, even back as far as the VRML standard \cite{Ferscha1999}. SecondLife in particular generated significant interest. Erickson et al. went so far as to say that a virtual conference hosted in SecondLife was 'fairly successful' \cite{Erickson2011}. For whatever reason however these systems have fallen out of favour with the public and research communities. It might be that the external viewpoint perspective, or the clunky internal viewpoint are a barrier to communication. \\
In contrast 'immersive' collaborative virtual environments place the user physically into the virtual scene. These systems are less scalable than online virtual communities which can take better advantage of distributed resources \cite{Grimstead2005a, Benford1998}. Goebbels et al. designed a taxonomy for what they termed simply CVE's. In their design they provide an excellent high level description of the ICVE as spaces which provide ``distributed collaborative teams with a virtual space where they could meet as if face-to-face, co-exist and collaborate while sharing and manipulating virtual data", crucially designed in a way to 'disburden' the users' senses \cite{Goebbels2003} and reduce fragmentation of the shared perceptual environment \cite{Roberts2005}. The ability to collaborate in such systems extends even to closely coupled physical tasks \cite{Roberts2012}.\\
There are various technology systems which demonstrate heightened immersion and presence in a virtual environment while allowing interaction between parties who may or may not be physically co-located \cite{Murray2009a}. Attempts at bringing people together in VR extend back to the early days of the technology and systems such as DIVE \cite{Benford:1995hh}. \\
                        Bailenson et al. noted that while increased realism of such avatars increases co-presence it decreases self disclosure \cite{Bailenson2006}. Such systems seem to compromise social interaction even as the realism increases. Avatar representations increased in fidelity and eventually it became possible to share avatar representations of participants wearing body tracking \cite{Schreer2005} and eye tracking equipment \cite{Garau2001, Garau2003}. This enabled tracked, viewpoint independent interaction with reconnection of eye gaze cues within VR in a spatially faithful way \cite{Roberts2003, Murray2007, Steptoe2008, Steptoe2009, Steptoe2010b}.\\
       It is also possible (at least in research implementations) to fully reconstruct people as 3D models, and connect more than two users together utilising life-size multi-view video \cite{Roberts2015, Grau2007, Divorra2010, Gross2003, Beck2013}. These 3D video representations of users can share a virtual space in which spatial and directional information are maintained in a natural way \cite{Roberts2015, Wolff2008}. \\
       Fairchild et al. have developed a system for ICVE and desktop based on earlier work by Duckworth et al. \cite{Duckworth:2012tx}, adapted for use in the Telethrone system \cite{Fairchild2016}.
            \textbf{Eye tracking in ICVE}
                More recently, advanced eye tracking technologies allow the recording and replaying of accurate eye gaze information\cite{Ciger2004a, Steptoe2009, Steptoe2008} alongside information about pupil dilation toward determination of honesty and social presence \cite{Steptoe2010a}.
            Heldal found that collaborative tasks manifested fewer disturbances due to ``misunderstandings of reference or action" when using more immersive systems \cite{Heldal2005}.
        \subsubsection{Situated Displays}
            Between the complexity of ICVEs and the more ubiquitous screen based VC technology there now exist so-called situated displays.\\
    Conversation does not exist in isolation, but rather in the context of semiotic resources. Participants make constant reference to the surrounding assets through mutual orientation, gesture, and diverse cues \cite{Goodwin2000a}.
            So called situated displays seek to embed the represented participant within the spatial and contextual framework of the conversation such that the referential cues are better supported. This has many implications, but chief amongst these is support for a spatially faithful conversational environment supportive of gaze \cite{Pan2014a, Oyekoya2012, Pan2012a, Zhang2013a}.\\
                    Such displays place a representation of the remote user into a space, theoretically allowing all participants to physically interact with the `contextual configurations' around them \cite{Goodwin2000a}. This is a relatively new field of research.  These could include the aforementioned telepresence robots \cite{Lee2011b, Sakamoto2007, Tsui2011}, head in a jar implementations such as SphereAvatar \cite{Oyekoya2012, pan2014comparing, Pan2012a},  and Gaze Preserving Situated Multi-View Telepresence System \cite{Pan2014a}.   Sphereavatar \cite{Oyekoya2012} demonstrates that there are problems with accurate mapping, distortion, and projection, and movement of the captured actors outside of very tight bounds.\\
                     Telehuman brings the whole body of a standing remote user into a space via a cylindrical display with a single tracked observer viewpoint \cite{Kim2012}.\\
            \subsubsection{Retro-reflective Projective Technology (RPT)}
Retro-reflective materials such as Chromatte(tm) cloth reflect light back along the angle of incidence. An everyday application of such material is high visibility jackets.\\
Inami et al. described the first use of RPT in 2000 with their visuo-haptic display \cite{inami2000visuo}. This system used head mounted projectors to augment RPT shapes in the real world. Krum et al. describe the REFLCT system \cite{Krum2012} which uses large retro-reflective surfaces to ``provide[s] users with a personal, perspective correct view of virtual elements that can be used to present social interactions with virtual humans''. They use helmet mounted projectors and describe a military training application in a large volume which allows faithful transmission of attention and gestures from the virtual to the real. They also briefly describe augmenting a facial mannequin by projecting onto RPT adhered to the surface. They point out that the optical characteristics of the material maintain polarisation and so could support passive stereoscopy.\\
Tachi describes an augmented reality system where a helmet mounted projector  places an image of a remote human onto a robot. This is a system they describe as Teleexistance RPT \cite{Tachi2003, Tachi2004}. This system demonstrates a single user, wearing a head mounted projector, viewing only a head which is captured from a single viewpoint. It is however enabled for motion by means of robotics under the Chromatte cloth like the hybrid robotic systems described earlier.\\
Hua et al. demonstrated elements of 'SCAPE', a tracked head mounted projection system surrounded by RPT cloth which could surround multiple physical users in a shared immersive experience \cite{Hua2004}. Hypothetically this system could be extended to include what they term 'passive remote users' projected into the views of the co-located and immersed users. The set-up however is quite complex, and involves wearing headgear, so is again less suitable for informal meetings.\\
Surman et al. discuss a potential development of their retroreflection based auto-stereoscopic display (which head tracks a single user to present a depth enabled stereo pair). They suggest that multiple users could address a large screen, but think that their stereopsis would break down and the system would be extremely challenging \cite{JSID:JSID295}.\\
Although less pertinent to this research there are other interesting deployments using RPT. Darken et al. and later Hahn in his PhD thesis in the same group describe a novel system which mounts a chromakey LED light ring and camera on a HMD \cite{Darken2003}. The camera takes in a mocked up physical cockpit and windows which are made of Chromatte cloth. This video image is very easy to segment to replace the green-screen windows with a simulated external view. The pilot trainee sees their own hands and the cockpit instrument panels (albeit in monoscopic video), alongside the generated external view. This augmented reality is much cheaper to deploy than a full simulator set-up.\\
\subsubsection{Furniture as a Mediating Display}
        Paul Sermon experimented with projection onto beds in his telematic dreaming work \cite{Sermon2000}.\\
Room2room from Microsoft Labs \cite{Pejsa2016} demonstrates the utility of projecting onto furniture by building upon their previous automatic projection mapping research \cite{Jones2014} using their Kinect system and projectors. This system is seen in Figure \ref{fig:room2room}. Their system provides spatially correct viewing through reconstruction but only on a point to point basis with a single user at each end. 

        \subsubsection{Augmented Reality Collaboration}
            Lehment at al described what they term a ``consensus reality" for head mounted AR in which they compute the correct locations for remote participants as 3D video representations overlaid on the real \cite{Lehment2014}.

\subsubsection{Holography and Volumetric}
Blanche et al. have done a great deal of research into holographic and volumetric displays using lasers, rotating surfaces, and light field technology   \cite{Blanche2010,tay2008updatable}. They are actively seeking to use their technologies for telepresence and their work is very interesting.\\
Similarly Jones et al. ``HeadSPIN" is a one-to-many 3D video teleconferencing system \cite{jones2009headspin} which uses a rotating display to render the holographic head of a remote party. They achieve transmissible and usable framerate using structured light scanning of a remote collaborator as they view a 2D screen which they say shows a spatially correct view of the onlooking parties.\\
Eldes et al. used a rotating display to present multi-view autostereoscopic projected images to users \cite{eldes2013multi}.\\
Seelinder is an interesting system which uses parallax barriers to render a head which an onlooking viewer can walk around. The system uses 360 high resolution still images which means a new spatially segmented view of the head every 1 degree of arc. They claim the system is capable of playback of video and this head in a jar multi-view system clearly has merit but is comparatively small, and as yet untested for telepresence \cite{Yendo2010}.\\
These systems do not satisfy the requirement to render upper body for the viewers and are not situated (as described soon).\\

        \subsection{Support for Less Formal?}
        
Outside of the sphere of simple webcam systems such as Zoom and their ilk there has been little attention invested in advanced telecommunication tools for informal settings. This is exactly the kind of thing necessary to enable home working in the future.\\
Slovak et al. describe a group-to-group videoconferencing system called GColl in which they highlight mutual gaze support and modest technical requirements \cite{Slovak2009}. GColl is a monitor based system which seeks to align mutual gaze by rendering to a small window within 5 degree of the mounted camera. It seems to address the right problems but is a questionable development over and above a careful videoconferencing set-up. De Greef and Ijsselsteijn describe a system which provides collaborative working tools alongside video and audio conferencing for the home \cite{DeGreef2001}. Judge and Neustaedter examined how 21 families used videoconferencing in a home setting and found that requirement for planning and availability for using the tools inhibited their use. They suggest an 'always on' system might better mediate availability \cite{Judge2010}. \\
Interestingly Lottridge et al. identified that it is the empty reflective moments during mundane activity which might benefit most from the ability to strike up intimate conversations between separated couples \cite{Lottridge2009}. Whether and how much this supposition is analogous to the so-called `water cooler meeting' moments in business remains an open question but is perhaps worth bearing in mind.\\
There is, of course, Room2Room which provides a far better informal home experience, but is inherently one to one \cite{Pejsa2016}. The same is true of 'Holoportation', a reconstructed and tracked 3D video based system designed by Microsoft for head mounted display and seemingly aimed at the home market \cite{orts2016holoportation}. Similarly utilising Microsoft hardware, but on a tight budget, HomeProxy uses a simple combination of consumer devices to render a reconstructed remote partner to a cabinet style display on a desk \cite{Tang2013}. Users reported that they appreciated the look and feel of the HomeProxy system which is designed to fit in sympathetically to home furnishings.\\
Gaver explains that ''the “space” created by audio-video technologies is significantly different from spaces as found in hallways, offices or meeting rooms", conveying a limited subset of visual and auditory information, preventing movement and exploration, and is often arbitrary and discontinuous. He says that ''these properties shape the possibilities.. for collaboration." \cite{Gaver1992}.\\
In a work setting Fish et al. said (in 1993) that ``informal communication cuts across organisational boundaries and often happens spontaneously." They reference research which suggests that informal communication is more frequent, expressive, and interactive \cite{Fish1993}. Fish went on to look at early telepresence systems which might support informal, but by any standard these systems are too clunky to be deemed capable of such. Although this has been recognised as an issue for decades it could nonetheless seem that less formal business meetings are of insufficient utility to attract the technology and research to better support them. Computer Supported Collaborative Working (CSCW) is a well known acronym  but there seems to be no single point of entry to research into less formal systems. This could be a function of the technology being better suited to deployment in a controlled formal setting. \\
The paper ``Ad hoc versus established groups in an electronic meeting system environment" turns out to be using the other definition of ad-hoc meetings which is meetings convened from random members with no past and no future together. The study is also dated being from 1990, but still provides insight. In the paper Dennis et al. reviewed what little research existed at the time noted that meeting outcomes varied between studies as to whether ad-hoc or more established and formal meetings were more productive \cite{Dennis1990}.\\
%Reconnection of naturalistic non-verbal cues bolsters turn passing \cite{Gu2006}, trust, empathy, and rapport between co-located, and telepresent users \cite{Bohannon2013}. The Telethrone aims to address this requirement for a simple, deployable, pervasive, group telecommunication system with spatial, non-verbal cue support. 
            Fayard and Weeks examined the affordances of informal interaction (indeed exactly the `water cooler meetings' that the BBC were talking about) \cite{Fayard2007}. They identified: proximity, privacy, and legitimacy as key affordances which should be supported, and highlighted: functional centrality, semi-enclosure, reciprocal visibility, easy access and egress, multiple shared resources as important further affordances of the environment. The affordance of legitimacy is interesting. This means that the location of the informal meeting should support the feeling that it is legitimate to be there and strike up a conversation. 
            A potential model example is explored in Horgan's book `Excellence by design: Transforming workplace and work practice'. `The LX common' was a semi communal space for informal meetings. It was semi-enclosed to give some privacy, but located on a busy thoroughfare and housed the photocopiers, printers, shared reference material, kitchen etc.  People who passed felt free to listen and occasionally join in. Three rules evolved from the research; Traffic through the common was acceptable at any time, anyone was free to join any meeting in the common, anyone was free to leave any meeting in the common at any time \cite{horgen1999excellence}.\\
            Supporting spatial aspects such as mutual gaze or `full gaze awareness'' \cite{Monk2002} in this way normally demands large purpose built installations which poorly support casual or ad hoc meeting paradigms \cite{Schroeder2001, Wolff2008}. It seems clear that there is a justification for development of simple affordable technologies which better mediates communication over distance for less formal situations.   

INSERT


Multiple `narrow baseline' pairs can create stereo geometry reliably from a tightly coupled pair of cameras which are integrated with geometry from another pair with an independent viewpoint \cite{Okutomi:1993ez, Cooke2002, Cooke2002a}. \\
Alternatively there is the so-called ``wide baseline" approach in which a pair of cameras with a significant distance between them capture both sides of the face or body. Sadagic et al. demonstrated a system which brought 2 camera reconstructed \cite{Sadagic2001} people into a desk based collaborative space. The user addressing these two tele-present collaborators could move around in their chair while being presented with both passive stereo depth cues, and parallax cues appropriate to the desk environment. The system's main limitation was the necessity for the user who addressed the two desk screens to wear both stereo glasses and a camera on their head. This made their system unbalanced.  There are also problems with geometry and texture management using a wide baseline approach since the cameras are resolving for different lighting and views of the scene.\\
      Pan, Steptoe and Steed found similar results with their spherical display with a decrease in trust toward avatar mediated conversation when viewing 2D displays at oblique angles \cite{pan2014comparing}.\\
Microsoft Kinect sensors demonstrate effectiveness in real-time scanning with between 1 and 5 deployed in research systems \cite{Mekuria2013}. Multiple Kinect v1 sensors interfere with one another, and though this problem is partially addressed with v2 hardware there are constraints with temperature drift causing frequency shifts which allow interference to creep in. There are workarounds for this issue but no completely reliable implementation.\\

                For some time it has been possible to create geometric models of the human form using a technique called ``shape from silhouette" \cite{Allard2006, Petit:2008tva,Matusik2000,Starck2008,Franco2003,Baumgart1975,Laurentini1994a,Grau2007,Waizenegger2011,Franco2009,Feldmann2009,Cooke2000, Starck2003}\\
                Similar effect can also be accomplished for the human shape using depth cameras such as the Microsoft Kinect v2 \cite{Maimone2011}, and indeed this seems now to be the prevailing method. Sensors in this vein continue to improve.\\
       Structured light systems provide still another technique, back as early as the 1980's \cite{Hu:1986ww}, and as recently as the Microsoft Kinect v1 sensor. Deformations in projected visible or infrared patterns can be compared to the known baseline and shape reverse engineered from the differences. 3D-Board is an interesting example since two standing collaborators can interact through touch with a digital whiteboard which also forms the perceived barrier between them \cite{Zillner2014}.\\
       \textbf{Rendering and telepresence}
       
                        Cooke et al. investigate the difference between so-called narrow and wide baseline camera capture for dealing with the more complicated elements of reconstructed geometry such as hands which gesture. They maintain that multiple stereo camera pairs (each narrow baseline) arranged as a wide baseline system, with post processing to remove artifacts is of most use \cite{Cooke_image-basedrendering}.\\
                        Petit et al. adopt a wide baseline system with conventional green screen background segmentation and get good 3D video results. Because they are using a chroma based technique for their silhouette system they cannot exclude objects in the scene by `training' the system \cite{Petit2010}.\\
Kuster et al. use a depth based approach and have a compelling demonstrator which uses what they call an ``anisotropic transparent background back projection foil''. They rear project a Kinect reconstructed head and upper body in stereoscopic 3D onto a transparent film suspended so as to appear to bring the remote person to the edge of a table. This brings the 3D avatar into a space at 15 frames per second across significant network distances but is again peer to peer\cite{Kuster2012a}.\\
Similarly Maimone and Fuchs use large tiled displays and Kinect sensors but attempt to 'merge' two spaces by scanning both the person and the remote space to create perspective correct viewing through a virtual window into the other space. Without multi-view this system is one to one, although they demonstrate multiple people in the background of the space \cite{Maimone2011a}.\\
       The Fusion4D system successfully captured complex and challenging scenes such as dancers with flowing clothing for playback from arbitrary angles \cite{Dou2016}.\\
withyou is an experimental capture and playback system which uses the Octave multi-modal suite. This system uses wide baseline single cameras and CPU based shape from silhouette reconstruction alongside a novel network transport system to send a full 3D video polygonal hull to another rendering location \cite{Roberts2015}. Previous tests on the system suggested that the capture and playback made it possible to judge the eye gaze of the reconstructed subjects to within normal social/biological limits \cite{Roberts2013}.\\
A particularly well developed example is the blue-c system which enables telepresence through multi-camera 3D video. The users at multiple locations are scanned and transmitted as a point cloud captured during projector blanking frames \cite{Gross2003}. The system is flexible enough to use either CAVE style surround or a projected screen style arrangement.\\
The ViPiD system interestingly uses Chromatte in its intended role to improve segmentation for their capture system \cite{Klie2006}. \\
Schreer at al demonstrate a capable multi camera capture system suitable for small group telepresence, though its technical overhead is high \cite{Schreer:2008ty}. They discuss the need for multi-view capable screens only insomuch as they agree one must be developed for the system.\\
Maimone at al demonstrate a system uses a HMD system to project a reconstructed person into the correct seat at a table in augmented reality \cite{Maimone2013}.\\
                        
%{matusik2000image,     title={Image-based visual hulls},     author={Matusik, Wojciech and Buehler, Chris and Raskar, Ramesh and   Gortler, Steven J and McMillan, Leonard},     booktitle={Proceedings of the 27th annual conference on Computer   graphics and interactive techniques},     pages={369--374},     year={2000},     organization={ACM Press/Addison-Wesley Publishing Co.}

%http://gl.ict.usc.edu/Research/PicoArray/
%http://gl.ict.usc.edu/Research/CNNHologram/
            

\subsection{Support for Dynamic Meetings}
Voida et al. discuss natural sub grouping in partially distributed teams \cite{Voida2012}. These `fault lines' within meetings are a natural component of more informal meeting structure with groups potentially forming and dispersing naturally over time. \\           
Greenburg and Jerald found in group interaction with trainers that members often changed their seating position to facilitate discussion goals \cite{greenberg1976role}. This may not be true of all group meetings but it is under supported in telepresence systems.\\
            This dynamic reorganisation of meetings is potentially an affordance for ad-hoc and casual meeting styles. It demands the ability to reorganise chairs such that they still have faithful views onto remote participants. There is no support for this found in literature.\\
 %  Bailenson et al. found in immersive VR that participants exhibited natural personal space when moving around a virtual avatar / agent provided that the remote representation exhibited natural eye gaze interaction \cite{bailenson2003interpersonal}.           
%\input{scrap}

Both Second Life and more recently the game Fortnite can be seen as precursors. Second Life should rightly be viewed as the first serious attempt at a metaverse, and was being described as such by users as early as \href{https://nwn.blogs.com/nwn/2003/06/the_early_creat.html}{2002/2003}. It broke through into academic research and several Universities bought `digital land' and started talking about using the platform for education \cite{sermon2008they, emp2006putting, kirriemuir2008spring, sant2009performance}. Businesses began to develop and showcase virtual products. Interest in the platform \href{https://trends.google.com/trends/explore?date=all&q=second\%20life}{waned by 2010}, although the platform is still operational and under development with \href{https://danielvoyager.wordpress.com/category/second-life-stats/}{around 40k} median concurrent users. Surprisingly this is similar to the 2007 peak use, but this is in the context of other platforms now boasting considerably faster growth (more later).\\
Epic games Tim Sweeney \href{https://venturebeat.com/2017/05/15/epic-games-tim-sweeney-fears-the-metaverse-will-be-a-proprietary-technology/}{attaching the word metaverse} to social events within fortnite in 2017.\\
``You’re seeing the beginning components of the Metaverse coming together now..'' - Tim Sweeney\\
Clearly he ignored the extensive work of Second Life here, but fortnite demonstrated a new level of social engagement boasting millions of concurrent users in a single space for major events in the game. Most interestingly events outside of the game logic emerged, with concerts drawing over \href{https://www.epicgames.com/fortnite/en-US/news/astronomical}{10M users on occasion}. This has kickstarted a new round of academic interest in the phenomenon \cite{marlatt2020capitalizing}. A new era of microtransactions for in game assets has begun, thought this is constrained to the walled economic garden within Epic's servers.
              \section{Risks}
              \subsection{Network latency}
            Bradner \& Mark established that latency matters in remote telecollaboration \cite{Bradner:2002:WDM:587078.587110}, while an extensive body of work notes that system latency impacts key factors such as interruptability \cite{Avrahami2007}. Gergle et al. found that remote collaborating pairs of people were tolerant of delays in visual feedback up to 939ms, after which shared task performance suffered considerably \cite{Gergle2006}. Overall it is clear that for communication systems, especially those with potential for shared or collaborative task, those aspects of the system latency which can be controlled and/or minimised should be.\\
             Early research around COVEN and DIVE described the benefits of multicast for multiparty collaborative technology systems. While multicast still has advantages in latency critical set-ups the bandwidth/throughput has to a degree `caught up', while the technical demands of a wide area multicast network remain the same. The DIVE system has a standalone WAN application layer network transport called DIVEBONE which attempted to address this issue \cite{Greenhalgh2001a}. Steed et al. describe a hybrid multi/uni cast system which allows local multicast with unicast on the WAN \cite{Greenhalgh2001}.\\
              Bradner used social impact theory to suggest  it is important to either say the distally located person is geographically remote to emulate real systems, or else explain they are in the next room to exclude this factor \cite{Bradner:2002:WDM:587078.587110}, while an extensive body of work notes that system latency impacts key factors such as interruptibility \cite{Avrahami2007}.\\
         Gasparello et al. \cite{Gasparello:2011in} then later Moore et al. \cite{Moore:2010jt} worked within the wider research group on novel network transport systems for synchronised 2D and 3D video information to support telepresence reconstruction in the later withyou system \cite{Roberts2015}. \\
       When Lamboray extended their blue-c 3D video system they considered that latency of up to 200ms was acceptable so long as jitter (changes in latency) remained low \cite{Park1999, lamboray2005data}.
       \subsection{Technology for it's own sake}
       

\section{Post `Meta' metaverse}
The current media around ``metaverse'' has been seeded by Mark Zuckerberg's rebranding of his Facebook company to `Meta', and his planned investment in the technology. The second order hype is likely a speculative play by major companies on the future of the internet. There has been a reactive pushback against this by the wider tech community who are concerns about monetisation of biometrics. \href{https://www.coindesk.com/layer2/2022/01/19/meta-leans-in-to-tracking-your-emotions-in-the-metaverse/}{Observers do not trust} these `Web2' players with such a potentially powerful social medium. It is very plausible that this is all just a marketing play that goes nowhere and fizzles out. It is by no means clear that people want to spend time socialising globally in virtual and mixed reality. These major companies are  making an asymmetric bet that if there is a move into virtual worlds, then they need to be stakeholders in the gatekeeping capabilities of those worlds.\\ 
Meta,Disney plus, Sportswear manufacturers\\

\href{https://medium.com/kabuni/fiction-vs-non-fiction-98aa0098f3b0}{Can enough be done to prevent abuse?}

It seems like there are four major interpretations of the word.\\

Facebook have recently rebranding their parent company as `Meta' and they are aggressively promoting ``The Metaverse'' as a shared social VR space, chiefly of their design. In Stephenson's `Snow Crash' the Hero Protagonist (drolly called Hiro Protagonist) spends much of the novel in a dystopian virtual environment called the metaverse. It is unclear if Facebook is deliberately embracing the irony of aping such a dystopian image, but certainly their known predisposition for corporate surveillance, alongside their attempt at a global digital money is ringing alarm bells.\\
The Grayscale investment trust \href{https://grayscale.com/wp-content/uploads/2021/11/Grayscale_Metaverse_Report_Nov2021.pdf}{published a report} which views Metaverse as a potential trillion dollar global industry. Such industry reports are given to hyperbole, but it seems the technology is becoming the focus of technology investment narratives.
\subsection{Mixed reality as a metaverse}
\href{https://docs.microsoft.com/en-us/windows/mixed-reality/design/spatial-anchors}{Spatial anchors} allow digital objects to be overlaid persistently in the real world. With a global `shared truth' of such objects a different kind of metaverse can arise.
One such example is the forthcoming \href{https://avvyland.com/}{AVVYLAND}.

Peleton as a metaverse?

\section{Digital Land Metaverses}
One of the most intuitive ways to view a metaverse is as a virtual landscape. This is how metaverse was portrayed in the original Neal Stephenson use of the word. 
\subsection{Legacy Web2}
\subsubsection{Secondlife}
\subsubsection{Roblox}
\subsubsection{Minecraft}
\subsubsection{Fortnite}
\subsection{The new stuff}
\subsubsection{Decentraland}
JPMorgan Chase, the investmemt bank, ahas just opened a `lounge' in the Decentraland metaverse. This coincided with a \href{https://www.jpmorgan.com/content/dam/jpm/treasury-services/documents/opportunities-in-the-metaverse.pdf}{major report} on the potential opportunities.
\subsubsection{Sonmiumspace}
\subsubsection{The Sandbox}
%\lipsum[50]
\section{Global enterprise perspective}
Meta(Facebook)

Nvidia Omniverse is \href{https://www.nvidia.com/en-us/omniverse/creators/}{free for creators}, Unity etc \\

Microsoft have just bought Activision / Blizzard for around seventy billion dollars. This is have been communicated by Microsoft executives as a ''Metaverse play'', leveraging their internal game item markets, and their massive multiplayer game worlds to build toward a closed metaverse experience like the one Meta is planning.
This builds on the success of early experiments like the Fornite based music concerts, which attracted millions of concurrent users to live events.

\section{NFT as metaverse narrative}
Within the NFT community it is normalised to refer to ownership of digital tokens as participation in a metaverse. 
This CNBC article highlights the confusion, as this major news outlet refers to \href{https://www.cnbc.com/2022/01/16/walmart-is-quietly-preparing-to-enter-the-metaverse.html}{Walmart prepares to offer NFTs}'' as an entry ``into the metaverse''.
%\lipsum[50]
\section{MMORG games and NFTs}
Traditional gamers have pushed back on the seemingly useful idea of integrating NTFs with traditional games. This may be in part because Ethereum mining has kept graphics card prices high for a decade.

\href{https://www.prnewswire.com/news-releases/hbar-foundation-and-ubisoft-partner-to-support-growth-of-gaming-on-hedera-network-301474971.html}{HBAR partnerships}
The \href{https://twitter.com/justinkan/status/1491270239967154178}{following text} is from Justin Kan, co-founder of twitch:
\begin{fminipage}{\textwidth}
NFTs are a better business model for games. Many gamers seem to be raging hard against game studios selling NFTs. But NFTs are also better for players. Here’s why I think blockchain games will be the predominant business model in gaming in ten years. NFTs are a better business model for funding games . Example: recently I invested in a new web3 game SynCityHQ. They are building a mafia metaverse and raised \$3M in their initial NFT drop.\\
NFTs give studios access to a new capital market for raising capital from the crowd.NFTs can be a better ongoing model for games. Web3 games will open economies, and by building the games on open and programmable assets (tokens + NFTs) they will create far more economic value than they could from any one game. Imagine Fortnite, but other developers can build experiences on top of the V-Bucks and skins. Epic would get a royalty every time any transaction happens. As big as Fortnite is today, Open Fortnite could be much bigger, because it will be a true platform. NFTs are better for gamers Allowing gamers to have ownership of the assets they buy and earn in game allows them to participate in the potential growth of a game. It lets gamers preserve some economic value when they switch to playing something new. But what about the criticisms of NFTs?
Here are my thoughts on the common FUDs: "It’s just a money grab on the part of the studios!"
Game studios already switched over to the model of selling in-game items, cosmetics, etc to players long ago. But currently the digital stuff players are buying isn’t re-sellable. NFT ownership is strictly better for players. "The games aren’t real games." This reminds me of the criticism of free-to-play in 2008, when the games were Mafia Wars / FarmVille. We haven’t had time for great developers to create incredible experiences yet. Everyone investing in games knows there are great teams building. "Game NFTs aren’t really decentralized because they rely on models / assets inside centralized game clients."
Crypto is as much a movement as it is a technology. Putting items on a blockchain is what gives people trust that they have participatory ownership...which make people willing to buy in to the game. These assets are “backed” by blockchain.
The fact that these item collections are NFTs will make other people willing to build on top of them. "NFTs are bad for the environment." Solana and L2s solve this. NFT games are better for players and for game developers. Like the free-to-play revolution changed gaming, so will blockchain. The games of the future will be fully robust, with open and programmable economies.
\end{fminipage}




\label{behaviours}
\begin{itemize}
\item As a user I want to select a digital asset I find in the AR/VR world and then be offered an option to purchase the asset so I can look at it in my own spaces.
\item As a user I want to click on a digital asset I find in the AR app and be given the opportunity to buy it as a rare digital representation so that only I and a few others are provably certified to own.
\item As a user I would like to transfer economic value to people and entities I meet in the metaverse such that it is agreed by all parties quickly that value has been provably transferred.
\item As a user I would like to access an online marketplace in the metaverse where I swap and trade digital assets with other users so that I continue to feel engaged.
\item As a user I would like to create content (inside or outside of the metaverse) so I can take it to metaverse and monetise it.
\item As a content creator or influencer I would like to engage with live audiences within the metaverse, and moneytise my opinions and knowledge in real-time. I would like to have a way to split this money with co-collaborators in real time.
\end{itemize}
\section{Crypto metaverses}

\href{https://naavik.co/business-breakdowns/axie-infinity/#axie-decon=}{Report on Axie Infinity}

\href{https://www.thesun.co.uk/tech/17348918/pavia-metaverse-cardano-crypto-game/}{Pavia Metaverse}
Probably the best example in the market at this time with connecting users with one another through blockchain is \href{https://lightnite.io/}{Satoshis Games `Litenite'}. Litenite is a `battle royale' game which allows users to earn Satoshis through the Lightning network.\\
Similarly, and potentially more significantly, Zebedee have brought Lightning based micropayments to \href{https://zebedee.io/infuse/}{Counter Strike}, which adds a financial layer directly to eSport, itself a multi billion pound global industry.\\
The Zebedee model is interesting in that they provide simple onboarding, and management solutions, for gaming and metaverse application developers. There is doubtless an opportunity to utilise their business model in the proposed stacks in the paper, but it would be at odds with the free and open source product methodology in this paper. Their CTO said the following in a \href{https://lightningjunkies.net/lightning-address-making-lightning-user-friendly-lnj052/}{recent podcast}:\\
``We all had very similar ideas around being able to put Lightning network capabilities into games. You're essentially putting real value inside of the game. So whether it's a point inside of the game or as a real end game economy, as a game developer, you don't have to worry about the mechanics around it.\\
You can use a real life currency: the money that exists in the world and that value carries regardless of which game you're in, regardless if you're in the real world or inside of a virtual world, like a game. We just think that vision is just sort of opening. Now there's so much more, if you extrapolate it into the Meatverse we would love for Zebedee to be a big platform provider and enabler for a lot of these.''\\
\href{https://spellsofgenesis.com/}{Spells of Genesis} is a long running card RPG trading game on mobiles which allows ``ownership'' of items and cards through non-fungible token ecosystems.\\
There are also hundreds of casinos which operate within and even on blockchain networks. These feel out of scope as they are a different and somewhat regulated offering.

\subsection{Immersive and third person XR}
In considering the needs of business to business and business to client social VR is it useful to compare software platforms:
\subsubsection{Second Life}
Notable because it's the original and has a decently mature marketplace.
%\lipsum[50]
\subsubsection{Spatial}
\begin{itemize}
\item Very compelling. Wins at wow.
\item Great avatars, user generated
\item AR first design
\item Limited scenes
\item Smaller groups (12?)
\item Limited headset support
\item Intuitive meeting support tools
\item No back end integration
\end{itemize}
\subsubsection{MeetinVR}
\begin{itemize}
\item Good enough graphics, pretty mature system
\item OK indicative avatars, user selected
\item VR first design
\item Limited scenes
\item Smaller groups (12?)
\item Quest and PC
\item Writing and gestures supported
\item Some basic enterprise tools integration
\item Bring in 3D objects
\item Need to apply for a license?
\end{itemize}
\subsubsection{Glue}
\begin{itemize}
\item Better enterprise security integration
\item Larger environments, potential for breakouts in the same space. Workshop capable
\item 3D object support, screen sharing, some collaborative tools
\item Apply for a license
\item Fairly basic graphics
\item Basic avatars
\item Quest and PC
\item Writing and gestures supported
\item Mac support
\end{itemize}
\subsubsection{Mozilla Hubs}
\begin{itemize}
\item Open source, bigger scale, more complex
\item Choose avatars, or import your own
\item Environments are provided, or can be designed
\item Useful for larger conferences with hundreds or thousands of members but is commensurately more complex
\item Quest and PC
\item Larger scenes within scenes
\end{itemize}
\subsubsection{FramesVR}
\begin{itemize}
\item Really simple to join
\item Basic avatars
\item Bit buggy
\item 3D object support, screen sharing, some collaborative tools
\item Quest and PC
\item Larger scenes within scenes
\item Runs in the browser
\end{itemize}
\subsubsection{AltSpace}
\begin{itemize}
\item Microsoft social meeting platform
\item Very good custom avatar design
\item Great world building editor in the engine
\item Doesn't really support business integration so it's a bit out of scope
\item Huge numbers (many thousands) possible so it's great for global events
\item Mac support
\end{itemize}
\subsubsection{Engage}
\begin{itemize}
\item Great polished graphics
\item Fully customisable avatars
\item Limited scenes
\item Presentation to groups for education and learning
\item PC first, quest is side loadable but that's a technical issue
\item BigScreen VR
\item Seated in observation points in a defined shared theatre
\item Screen sharing virtual communal screen watching, aimed at gamers, film watching
\item up to 12 user
\end{itemize}
\subsubsection{VRChat}
%\lipsum[50]
\subsubsection{NEOSVR}
\href{https://neos.com/}{Notable becasue} it's trying to integrate crypto marketplaces
\subsubsection{Meta Horizon Worlds \& Workrooms}
Horizon Worlds is the Meta (Facebook) meteverse, and Workrooms it's business offering and a subset of the ``Worlds'' global system. It is currently a walled garden without connection to the outside digital world, and arguably not therefore a metaverse.\\
The Financial Times \href{}{took a look} at their patent applications and noted that the travel is toward increased user behaviour tracking, and targeted advertising.\\
Facebook actually have a poor history on innovation and diversification of their business model. This model has previously been tracking users to target ads on their platform, while increasing and maintaining attention using machine learning algorithms. \\
It makes complete sense then to analyse the move by Meta into 3D social spaces as an attempt to front run the technology using their huge investment capacity. Facebook have recently taken a huge hit to their share price. Nothing seems to changed in the underling business except Zuckerbergs well publicised shift to supporting a money losing gamble on the Metaverse. It is by no means that clear users want this, that Meta will be able to better target ads on this new platform, or that the markets are willing to trust Zuckerburg on this proactive move. \\
With all this said the investment and management capacity and capability at Meta cannot be dismissed. It is very likely that Meta will be able to rapidly deploy a 3D social space, and that it's development will continue to be strong for years. The main interface for Horizon Worlds is through the Meta owned and developer Oculus headset, which is excellent and reasonably affordable. It has been quite poorly received \href{https://kotaku.com/facebook-metaverse-horizon-worlds-vr-oculus-quest-2-cha-1848436740}{by reviewers} but will likely improve, especially if users are encouraged to innovate.
\subsubsection{Vircadia}
We have chosen Vircadia as our development platform for this investigation is it's a community supported free and open source project with some support for economic interaction.
\subsection{WebXR/WebGL/WebGPU}
\subsubsection{Playcanvas}
\href{https://hackmd.io/@XR/monetization}{Monetisation of WebXR}
\href{https://github.com/playcanvas/engine}{Playcanvas - Telefonica}
%\lipsum[50]
\subsection{Integration with web and game engines} (other integrations?) 
\section{User stories}
%\lipsum[50]
\section{Recommendations for value transfer}
There are many claims about what blochchain technologies can enable. In the tens of thousands of attempts at utility over the last decade there are surprisingly few chains able to claim any value at all, and those that can often have significant problems in other areas. It is not the intention of this analysis to poke at problems. The primary use case within the context of a shared social space (metavserse) is low friction transmission of value. Remember that interlocutors and entities might have globally different physical locations outside of the real world. What is needed is instant ``settlement'' of digital bearer instruments within the context of the digital environement. This is fortunately what Bitcoin was designed to do. Additionally it is highly possible that exchange of digital goods (art and objects and provable services) will be required. This is also in scope within this document.
\section{Bitcoin market gap}
Nobody is currently deploying the discussed technologies purely on Bitcoin because it's slower evolution is still catching up. This is a market gap and the following section shows how this might be done.
\subsection{Money}
%\lipsum[50]
\subsection{Identity proofs}
%\lipsum[50]
\subsection{Digital object tracking}
%\lipsum[50]
\subsection{Object transfer and trading}
%\lipsum[50]
\section{Risks}
\href{https://www.carnegieuktrust.org.uk/blog-posts/regulating-the-future-the-online-safety-bill-and-the-metaverse/}{Online regulations still apply}
