

   
\subsubsection{Reconstructed Viewpoint}
            Although it is possible to manipulate a 2D view of a remote participant to bring the camera, eye, and screen into correct alignment better results can be obtained by capturing a 3D representation of the remote user, then modifying that. Several groups have investigated stereo reconstruction of face and upper body, so-called immersive videoconferencing \cite{Atzpadin2004}. \\                   Multiple viewpoints from multiple cameras allow either playback from different cameras on a continuous basis, so-called free viewpoint TV \cite{Zhang2007}. This multiple camera approach can be extended with the use of light field to interpolate between real or virtual cameras \cite{Ott1993, Al-Saidi2009, doi:10.1117/12.854571}, or else algorithmic generation and display of a polygonal mesh through photogrammetry can be undertaken. This can now be performed fast enough to compute a continuous surface from a video stream with little latency \cite{Criminisi:2003ji}. Engineers from Google presented an efficient method for network transmission of light field scenes at Siggraph \cite{MichaelBroxton}.  These multi camera systems aim to create geometry for a single virtual viewpoint (such as a face in video conferencing), which is correct to align eye gaze. This is sometimes called``3D video", which is different to stereoscopic video used in cinema and commercial TV.\\
 A depth map, point cloud, then polygon mesh can be calculated for one side of a person from two adjacent cameras which have slightly different views of a subject.  This so-called `narrow baseline' technique examines sub-pixel disparities between the images \cite{Knoblauch2008}. \\