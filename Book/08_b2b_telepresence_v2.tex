
\section{Current telecollaboration}
\subsection{Video conferencing}
Video-conferencing has become more popular as technology improves, and with increasing demands for real-time communication across greater distances. The full effects of video-conferencing on human communication are still being explored. Video-conferencing is presumed to be a somewhat richer form of communication than email and telephone, but not quite as informative as face-to-face communication. \par
In this section we look at the influence of eye contact on communication and how video-conferencing mediates both verbal and non-verbal interactions. Facilitation of eye contact is a challenge that must be addressed so that video-conferencing can approach the rich interactions of face-to-face communication. This is an even bugger problem in the emerging metaverse systems, so it's important that we examine the history and trajectory.\par
There is a tension emerging for companies who do not necessarily need to employ remote meeting technology, but also cannot afford to ignore the competitive advantages that such systems bring. In an experiment preformed well before the 2020 global pandemic at CTrip, Bloom et al describe how home working led to a 13\% performance increase, of which about 9\% was from working more minutes per shift (fewer breaks and sick-days) and 4\% from more calls per minute (attributed to a quieter working environment) \cite{Bloom2015}. Home workers also reported improved work satisfaction and experienced less turnover, but their promotion rate conditional on performance fell. This speaks to a lack of management capability with such systemic change. It's clearly a complex and still barely understood change within business and management. \par
Due to the success of the experiment, CTrip rolled-out the option to work from home to the whole company, and allowed the experimental employees to re-select between the home or office. Interestingly, over half of them switched, which led to the gains almost doubling to 22\%. This highlights the benefits of learning and selection effects when adopting modern management practices like working from home. Increasingly this is becoming a choice issue for prospective employees, and an advantage for hiring managers to be able to offer it.\par
More recently enterprise collaboration systems provide rich document management, sharing, and collaboration functionality across an organisation. The enterprise ECS system may integrate collaborative video \cite{prakash2020characteristic}. This is for instance the case with Microsoft Teams / Sharepoint. This integration of ECS should be considered when thinking about social VR systems which wish to support business, value, and trust. It is very much the case that large technology providers are attempting to integrate their `business back end' systems into their emerging metaverse systems. Open source equivalents are currently lacking.
\subsubsection{Pandemic drives adoption}
The ongoing global COVID-19 pandemic is changing how people work, toward a new global 'normal'. Some ways of working are overdue transformation, and will be naturally disrupted. In the UK at least it seems that there may be `real appetite' to shift away from old practises. This upheaval will inevitably present both challenges and opportunities.\par
Highly technical workforces, especially, can \href{https://globalworkplaceanalytics.com/telecommuting-statistics}{operate from anywhere}. Little seems to have changed since then, until the recent pandemic forced the hand of global business. \par
If only a small percentage of companies allow the option of remote working, then they gain a structural advantage, enjoying benefits of reduced travel, lower workplace infection risk across all disease, and global agility for the personnel. Building and estate costs will certainly be reduced. More diversity may be possible. Issues such as sexual harassment and bullying may be reduced.  With reduced overheads product quality may increase. If customers are happier with their services, then over time this `push' may mean an enormous shift away from centralised working practises toward distributed working. \par
Technologies which support this working style were still in their infancy at the beginning of the pandemic. The rush to `Zoom', a previously relatively unknown and insecure \cite{aiken2020zooming} web meeting product, shows how naive businesses were in this space. \par
Connection of multiple users is now far better supported, with Zoom and \href{https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast}{Mircosoft Teams} alone supporting hundreds of millions of chats a day. This is a 20x increase on market leader Skype's 2013 figure of \href{https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast}{280 million} connections per month. Such technologies extend traditional telephony to provide important multi sensory cues.  However, these technologies demonstrate shortfalls compared to a live face-to-face meeting, which is generally agreed to be optimal for human-human interaction \cite{Wolff2008}.\par
While the research community and business are learning how to adapt working practises to web based telepresence \cite{oeppen2020human}, there remains little technology support for ad-hoc serendipitous meetings between small groups. It's possible that Metaverse applications can help to fill this gap, by gamification of social spaces.\par

\subsubsection{Mona Lisa Type Effects}
Almost all traditional group video meeting tools suffer from the so-called Mona Lisa effect describes the phenomenon where the apparent gaze of a portrait or 2 dimensional image always appears to look at the observer regardless of the observer's position \cite{Vishwanath2005, Anstis1969, Wollaston1824}. This situation manifests when the painted or imaged subject is looking into the camera or at the eyes of the painter \cite{Loomis2008, Fullwood2006}.\par
Single user-to-user systems based around bidirectional video implicitly align the user's gaze by constraining the camera to roughly the same location as the display. When viewed away from this ideal axis, it creates the feeling of being looked at regardless of where this observer is \cite{Moubayed2012, Vishwanath2005, Anstis1969, Wollaston1824}, or the ``collapsed view effect'' \cite{Nguyen2005} where perception of gaze transmitted from a 2 dimensional image or video is dependent on the incidence of originating gaze to the transmission medium. \par
Multiple individuals using one such channel can feel as if they are being looked at simultaneously, leading to a breakdown in the normal non-verbal communication which mediates turn passing \cite{Vertegaal2002}.    
There is research investigating this sensitivity when the gaze is mediated by a technology, finding that ``disparity between the optical axis of the camera and the looking direction of a looker should be at most 1.2 degrees in the horizontal direction, and 1.7 degrees in vertical direction to support eye contact" \cite{Van_Eijk2010, Bock2008}. It seems that humans assume that they are being looked at unless they are sure that they are not \cite{Chen2002}.\par
It seems that the effect is truely limited to 2D surfaces. A 3D projection surface (a physical model of a human) designed to address this problem completely removed the Mona Lisa effect \cite{Moubayed2012}.\par
To be clear, there are technological solutions to this problem, but it's useful in the context of discussing metaverse to know that this problem exists. It's known that there are cognitive dissonances around panes of video conference images. Metaverse offers the promise of solving this, making more natural interaction possible, but it's clearly a long way from delivering on those promises.

\section{What's important for human communication}
\subsection{Vocal}
In the following sections we will attempt to rigorously identify just what is important for our proposed application of business centric communication, supportive of trust, and thereby value transfer.\par
In his book `Bodily Communication' \cite{Argyle1988} Michael Argyle divides vocal signals into the following categories:
\begin{enumerate}
\item Verbal
\item Non-Verbal Vocalisations
\begin{enumerate}
       \item Linked to Speech
       \begin{enumerate}
         \item   Prosodic
         \item   Synchronising
         \item   Speech Disturbances
         \end{enumerate}
      \item  Independent of Speech
      \begin{enumerate}
        \item    Emotional Noises
         \item   Paralinguistic (emotion and interpersonal attitudes)
         \item   Personal voice and quality of accent
         \end{enumerate}
\end{enumerate}
\end{enumerate}               
Additional to the semantic content of verbal communication there is a rich layer of meaning in pauses, gaps, and overlaps \cite{Heldner2010} which help to mediate who is speaking and who is listening in multi-party conversation. This mediation of turn passing, to facilitate flow, is by no means a given and is highly dependent on context and other factors \cite{Kleinke1986}. Interruptions are also a major factor in turn passing.\par
This extra-verbal content \cite{Ting-Toomey2012} extends into physical cues, so-called `nonverbal' cues, and there are utterances which link the verbal and non-verbal \cite{Otsuka2005}. This will be discussed later, but to an extent, it is impossible to discuss verbal communication without regard to the implicit support which exists around the words themselves.\par
In the context of all technology-mediated conversation the extra-verbal is easily compromised if technology used to support communication over a distance does not convey the information, or conveys it badly. This can introduce additional complexity \cite{Otsuka2005}.\par
These support structures are pretty much lacking in metaverse XR systems. The goal then here perhaps is to examine the state of the art, and remove as many of the known barriers as possible. Such a process might better support trust, which might better support the kind of economic and activity we seek to engineer.\par
When examining just verbal / audio communication technology it can be assumed that the physical non-verbal cues are lost, though not necessarily unused. In the absence of non-verbal cues it falls to timely vocal signals to take up the slack when framing and organising the turn passing. For the synchronising of vocal signals between the parties to be effective the systemic delays must remain small. System latency, the inherent delays added by the communication technology, can allow slips or a complete breakdown of 'flow' \cite{katagiri2007aiduti}. This problem can be felt in current social VR platforms, though people don't necessarily identify the cause of the breakdown correctly. In the main they feel to the users like a bad ``audio-only'' teleconference.\par
With that said, the transmission of verbal / audio remains the most critical element for interpersonal communication as the most essential meaning is encoded semantically. There is a debate about ratios of how much information is conveyed through the various human channels \cite{Loomis2012}, but it is reasonable to infer from its ubiquity that support for audio is essential for meaningful communication over a distance. We have seen that it must be timely, to prevent a breakdown of framing, and preferably have sufficient fidelity to convey sub-vocal utterances. \par
In the context of social immersive VR for business users, this implies that much better microphones are important, and the system should support both angular spatialisation, and respond to distance between interlocutors.
\subsection{Nonverbal}
We have already seen that verbal exchanges take place in a wider context of sub vocal and physical cues. In addition, the spatial relationship between the parties, their focus of attention, their gestures and actions, and the wider context of their environment all play a part in communication \cite{Goodwin2000}. These are identified as follows by Gillies and Slater \cite{Gillies2005} in their paper on virtual agents.\par
\begin{itemize}
\item Posture and gesture
\item Facial expression
\item Gaze
\item Proxemics
\item Head position and orientation
\item Interactional synchrony
\end{itemize}

This is clearly important for our proposed metaverse application. Below we will examine these six areas by looking across the wider available research.

\subsubsection{Gaze}
Of particular importance is judgement of eye gaze which is normally fast, accurate and automatic, operating at multiple levels of cognition through multiple cues \cite{Argyle1988,Argyle1976,Argyle1965,Argyle1976,Argyle1969, Kendon1967,Monk2002}.\par
Gaze in particular aids smooth turn passing \cite{Hedge1978} \cite{Novick1996} and lack of support for eye gaze has been found to decrease the efficiency of turn passing by 25\% \cite{Vertegaal2000}.\par
There are clear patterns to eye gaze in groups, with the person talking, or being talked to, probably also being looked at \cite{Vertegaal2001} \cite{Langton2000}. To facilitate this groups will tend to position themselves to maximally enable observation of the gaze of the other parties \cite{Kendon1967}. This intersects with proxemics which will be discussed shortly.  In general people look most when they are listening, with short glances of 3-10 seconds \cite{Argyle1965}. %Novick et al. performed analysis gaze patterns utilised for on task hand-off, which is potentially useful for extension into shared task experiment design \cite{Novick1996}.\par
Colburn et al. suggest that gaze direction and the perception of the gaze of others directly impacts social cognition \cite{Colburn2000} and this has been supported in a follow up study \cite{Macrae2002}.\par
The importance of gaze is clearly so significant in evolutionary terms that human acuity for eye direction is considered high at ~30 sec arc \cite{Symons2004} with straight binocular gaze judged more accurately than straight monocular gaze \cite{Kluttz2009}, when using stereo vision. \par
Regarding the judgement of the gaze of others, Symons et al. suggesting that ``people are remarkably sensitive to shifts in a person's eye gaze'' in triadic conversation \cite{Symons2004}. 
This perception of the gaze of others operates at a low level and is automatic. Langton et al. cite research stating that the gaze of others is ``able to trigger reflexive shifts of an observer's visual attention'' and further discuss the deep biological underpinnings of gaze processing \cite{Langton2000}. \par  
When discussing technology-mediated systems, Vertegaal \& Ding suggested that understanding the effects of gaze on triadic conversation is ``crucial for the design of teleconferencing systems and collaborative virtual environments'' \cite{Vertegaal2002}, and further found correlation between the amount of gaze, and amount of speech. Vertegaal \& Slagter suggest that ``gaze function(s) as an indicator of conversational attention in multiparty conversations'' \cite{Vertegaal2001}. It seems like is we are to have useful markets within social immersive environments then support for natural gaze effects should be a priority.\par  
Wilson et al. found that subjects can ``discriminate gaze focused on adjacent faces up to [3.5m]'' \cite{Wilson2000}. This perhaps gives us a testable benchmark within a metaverse application which is eye gaze enabled. In this regard Schrammel et al. investigated to what extent embodied agents can elicit the same responses in eye gaze detection \cite{Schrammel2007}.\par       
Vertegaal et al. found that task performace was 46\% better when gaze was synchronised in their telepresence scenario. As they point out, gaze synchonisation (temporal and spatial) is `commendable' in all such group situations, but the precise utility will depend upon the task \cite{Vertegaal2002}.\par
There has been some success in the automatic detection of the focus of attention of participants in multi party meetings \cite{Stiefelhagen2001, Stiefelhagen2002}.  More recently, eye tracking technologies allow the recording and replaying of accurate eye gaze information \cite{Steptoe2009} alongside information about pupil dilation toward determination of honesty and social presence \cite{Steptoe2010}. It seems there are trust and honesty issues conflated with how collaborants in a virtual space a represented.\par               
In summary, gaze awareness does not just mediate verbal communication but rather is a complex channel of communication in its own right. Importantly, gaze has a controlling impact on those who are involved in the communication at any one time, including and excluding even beyond the current participants. Perhaps the systems we propose in this book need to demand eye gaze support, but it is clear that it should be recommended, and that the software selected should support the technology integration in principle.\par
\subsubsection{Mutual Gaze}
Aygyle and Cook established early work around gaze and mutual gaze, with their seminal book of the same title \cite{Argyle1976}, additionally detailing confounding factors around limitations and inaccuracies in observance of gaze and how this varies with distance \cite{Argyle1969, Argyle1988, Cook1977}.\par
Mutual gaze is considered to be the most sophisticated form of gaze awareness with significant impact on dyadic conversation especially \cite{Cook1977, Kleinke1986, Fagel2010}. The effects seem more profound than just helping to mediate flow and attention, with mutual eye gaze aiding in memory recall and the formation of impressions \cite{Bohannon2013}.\par
While reconnection of mutual eye gaze through a technology boundary does not seem completely necessary it is potentially important, with impact on subtle elements of one to one communication, and therefore discrimination of eye gaze direction should be bi-directional if possible, and if possible have sufficient accuracy to judge direct eye contact. In their review Bohannon et al. said that the issue of rejoining eye contact must be addressed in order to fully realise the richness of simulating face to face encounters \cite{Bohannon2013}.\par
Mutual gaze is a challenging affordance as bi-directional connection of gaze is not a trivial problem. It's perhaps best to view this as at the `edge' of our requirements for a metaverse.
\subsubsection{Head Orientation}
Orientation of the head (judged by the breaking of bilateral symmetry and alignment of nose) is a key factor when judging attention. Perception of head orientation can be judged to within a couple of degrees \cite{Wilson2000}.\par
It has been established that head gaze can be detected all the way out to the extremis of peripheral vision, with accurate eye gaze assessment only achievable in central vision \cite{Loomis2008}. This is less of use for our metaverses at this time, because user field of view is almost always restricted in such systems. More usefully, features of illumination can alter the apparent orientation of the head \cite{Troje1998}.\par
Head motion over head orientation is a more nuanced propostion and can be considered a micro gesture \cite{Boker2011}. Head tracking systems within head mounted displays can certainly detect these tiny movements, but it's clear that not all of this resolution is passed into shared virtual settings through avatars. It would be beneficial to be able to fine tune this feature within any software selected.\par
                    It is possible that 3D displays are better suited to perception of head gaze since it is suggested that they are more suitable for ``shape understanding tasks'' \cite{St_John2001}\par
                    Bailenson, Baell, and Blascovich found that giving avatars rendered head movements in a shared virtual environment decreased the amount of talking, possibly as the extra channel of head gaze was opened up. They also reported that subjectively, communication was enhanced \cite{Bailenson2002}. \par
                    Clearly head orientation is an important indicator of the direction of attention of members of a group and can be discerned even in peripheral vision. This allows the focus of several parties to be followed simultaneously and is an important affordance to replicate on any multi-party communication system. \par
\subsubsection{Combined Head and Eye Gaze}
Rienks et al. found that head orientation alone does not provide a reliable cue for identification of the speaker in a multiparty setting \cite{Rienks2010}. Stiefelhagen \& Zhu found ``that head orientation contributes 68.9\% to the overall gaze direction on average'' \cite{Stiefelhagen2002}, though head and eye gaze seem to be judged interdependently \cite{Kluttz2009}. Langton noted that head and eye gaze are ``mutually influential in the analysis of social attention'' \cite{Langton2000}, and it is clear that transmission of `head gaze' by any mediating system, enhances rather than replaces timely detection of subtle cues. Combined head and eye gaze give the best of both worlds and extend the lateral field of view in which attention can be reliably conveyed to others \cite{Loomis2008}.
\subsubsection{Other Upper Body: Overview}
While it is well evidenced that there are advantages to accurate connection of the gaze between conversational partners \cite{Argyle1969, Kleinke1986}, there is also a body of evidence that physical communication channels extend beyond the face \cite{Kleinke1986, Nguyen2009} and include both micro (shrugs, hands and arms), and macro movement of the upper body \cite{Ekman1993}. Goldin-Meadow suggests that gesturing aids conversational flow by resolving mismatches and aiding cognition \cite{Goldin-Meadow1999}.\par
                    In their technology-mediated experiment which compared face to upper body and face on a flat screen, Nguyen and Canny found that ``upper-body framing improves empathy measures and gives results not significantly different from face-to-face under several empathy measures'' \cite{Nguyen2009}. 
                    
The upper body can be broken up as follows:\par
\textbf{Facial}\\Much emotional context can be described by facial expression (display) alone \cite{Ekman1993, Chovil1991}, with smooth transition between expressions seemingly important \cite{schiano2004}. This suggests that mediating technologies should support high temporal resolution, or at least that there is a minimum resolution between which transitions between expressions become too 'categorical'. Some aspects of conversational flow appear to be mediated in part by facial expression \cite{ohba1998}. There are gender differences in the perception of facial affect \cite{Hofmann2006}.\par
\textbf{Gesturing} \\(such as pointing at objects) paves the way for more complex channels of human communication and is a basic and ubiquitous channel \cite{Iverson2005}.  Conversational hand gestures provide a powerful additional augmentation to verbal content \cite{Krauss1996}.\par
\textbf{Posture} \\Some emotions can be conveyed through upper body configurations alone. Argyle details some of these \cite{Argyle1988} and makes reference to the posture of the body and the arrangement of the arms (i.e. folded across the chest). These are clearly important cues. Kleinsmith and Bianchi-Berthouze assert that "some affective expressions may be better communicated by the body than the face" \cite{Kleinsmith2013}.\par
\textbf{Body Torque} \\In multi-party conversation, body torque, that is the rotation of the trunk from front facing, can convey aspects of attention and focus \cite{10.2307/40971262}.\par
In summary, visual cues which manifest on the upper body and face can convey meaning, mediate conversation, direct attention, and augment verbal utterances. \par
\subsubsection{Effect of Shared Objects on Gaze}
Ou et al. detail shared task eye gaze behaviour ``in which helpers seek visual evidence for workers' understanding when they lack confidence of that understanding, either from a shared, or common vocabulary'' \cite{Ou2005}.\par 
  Murray et al. found that in virtual environments, eye gaze is crucial for discerning what a subject is looking at \cite{Murray2009}. This work is shown in Figure \ref{fig:murrayeyegaze}.\par
It is established that conversation around a shared object or task, especially a complex one, mitigates gaze between parties \cite{Argyle1976} and this suggests the choice of task is an important factor when examining eye gaze in experimental groups. \par
\begin{figure}[!h]
\includegraphics[width=\linewidth]{murrayeyegaze.jpg}
\caption{Eye tracked eye gaze awareness in VR. Murray et al. used immersive and semi immersive systems alongside eye trackers to examine the ability of two avatars to detect the gaze awareness of a similarly immersed collaborator.}
\label{fig:murrayeyegaze}
\end{figure}                                       
\section{Psychology of Technology-Mediated Interaction}       
\subsection{Proxemics}
Proxemics is the formal study of the regions of interpersonal space begun in the late 50's by Hall and Sommers and building toward The Hidden Dimension \cite{Hall1969}, which details bands of space that are implicitly and instinctively created by humans and which have a direct bearing on communication.\par
Distance between conversational partners, and affiliation, also have a bearing on the level of eye contact \cite{Argyle1965} with a natural distance equilibrium being established and developed throughout, through both eye contact and a variety of subtle factors. Argyle \& Ingham provide levels of expected gaze and mutual gaze against distance \cite{Argyle1969}. These boundaries are altered by ethnicity \cite{Watson1966, Argyle1988} and somewhat by gender \cite{Bruno2013}, and age \cite{Slessor2008, Hofmann2006}.\par
Even with significant abstraction by communication systems (such as SecondLife) social norms around personal space persist \cite{Yee2007, 6790745, bailenson2003interpersonal}. Bailenson \& Blascovich found that even in Immersive Collaborative Virtual Environments (ICVE's) ``participants respected personal space of the humanoid representation''\cite{6790745} implying that this is a deeply held 'low-level' psychophysical reaction \cite{Blascovich2002}.\par
It is highly applicable to experimental design that the noted `social space' be considered throughout.\par
Maeda et al. \cite{Maeda2004} found that seating position impacts the level of engagement in teleconferencing. Taken together with the potential for reconfiguration within the group as well as perhaps signaling for the attention of participants outside of the confines of the group.\par
When considering the attention of engaging with people outside the confines of a meeting Hager et al. found that gross expressions can be resolved by humans long distances \cite{Hager1979, Argyle}. It seems that social interaction begins around 7.5m in the so-called `public space' \cite{Hall1969}.\par                
\subsection{Attention}
The study of attention is a discrete branch of psychology. It is the study of cognitive selection toward a subjective or objective sub focus, to the relative exclusion of other stimulae. It has been defined as ``a range of neural operations that selectively enhance processing of information'' \cite{carlston2013}. In the context of interpersonal communication it can be refined to apply to selectively favouring a conversational agent or object or task above other stimuli in the contextual frame.\par
Humans can readily determine the focus of attention of others in their space \cite{Stiefelhagen2001} and preservation of the spatial cues which support this are important for technology-mediated communication \cite{Sellen1992} \cite{Stiefelhagen2002}.\par
Symons et al. suggested that ``people are remarkably sensitive to shifts in a person's eye gaze'' in triadic conversation \cite{Symons2004}. Wilson et al. found that subjects can ``discriminate gaze focused on adjacent faces up to [3.5m]'' \cite{Wilson2000}\par
The interplay between conversational partners, especially the reciprocal perception of attention, is dubbed the perceptual crossing \cite{Deckers2013, Gibson1963}.\par
This is a complex field of study with gender, age, and ethnicity all impacting the behaviour of interpersonal attention \cite{Bente1998, Slessor2008, Argyle1988, Hofmann20061683, Pan2008a}.
Vertegaal has done a great deal of work on awareness and attention in technology-mediated situations and the work of his group is cited throughout the thesis \cite{Vertegaal1997}. As an example it is still such a challenge to ``get'' attention through mediated channels of communication, that some research \cite{Fels2000, Sellen1992} and many commercial systems such as `blackboard collaborate', use tell tale signals (such as a microphone icon) to indicate when a participant is actively contributing. Some are automatic, but many are still manual, requiring that a user effectively hold up a virtual hand to signal their wish to communicate.\par
Langton et al. cite research stating that the gaze of others is ``able to trigger reflexive shifts of an observer's visual attention''. Novick et al. performed analysis on task hand-off gaze patterns which is useful for extension into shared task experiment design \cite{Novick1996}.\par
Attention and the sensing of attention rely on the reconnection of non-verbal cues across the whole of the space in which communication occurs. \par%In the case of a dynamic informal meeting this area of communicational attention may extend beyond the bounds of the meeting.\par
%Functioning at a more complex level of cognition than simple attention is interpersonal behaviour. Some of this is derived from information provided by attention and that of others, but behaviour at this level can be considered to apply `modifiers' which reflect more complex drivers.\par
Regarding the attention of others, Fagal et el demonstrated that eye visibility impacts collaborative task performance when considering a shared task \cite{Fagel2010}.\par
\subsection{Behaviour}
Hedge et al. suggested that gaze interactions between strangers and friends may be different which could have an impact on research recruitment \cite{Hedge1978}. Voida et al. elaborate that prior relationships can cause ``internal fault lines'' in group working \cite{Voida2012}. %This concept of fault lines became increasingly pertinent as the research evolved and is explored in more detail later.\par
When new relationships are formed the “primary concern is one of uncertainty reduction or increasing predictability about the behavior of both themselves and others in the interaction \cite{berger1975}. This concept of smoothness in the conversation is a recurring theme, with better engineered systems introducing less extraneous artefacts into the communication, and so disturbing the flow less. In a similar vein the actor-observer effect describes the mismatch between expectations which can creep into conversation. Conversations mediated by technology can be especially prone to diverging perceptions of the causes of behaviour \cite{Jones1971}.% This may be especially prevelant in shared immersive environments where different subjective views of the same objective shared dataset exist \cite{Snowdon1995}.\par
Interacting subjects progress conversation through so-called `perception-action' loops which are open to predictive modelling through discrete hidden Markov models \cite{Mihoub2015}.\par
It may be that the perception-behaviour link where unconscious mirroring of posture bolsters empathy between conversational partners, especially when working collaboratively \cite{Chartrand1999}, and the extent to which posture is represented through a communication medium may be important.\par
Landsberger posited the Hawthorne effect \cite{Parsons922}. Put simply this is a short term increase in productivity that may occur as a result of being watched or appreciated. This has broader implications for experimentation where tasks are observed or are perceived to be observed.\par
The impression of being watched changes gaze patterns during experimentation, with even implied observation through an eye tracker modifying behaviour \cite{risko2011eyes}.\par
There is much historic work describing ``the anatomy of cooperation" \cite{Kollock1998} and this may be useful in analysing experimental data around common collaborative tasks.\par
%Blascovich proposes a model ``within technology-mediated and/or determined contexts" \cite{Blascovich2002}. This work suggests that some catagories of behaviour are more important than others \cite{PARKS} and deserve different degrees of support, though again this is context dependent.\par
%Johnson discusses the importance to cognition of represtations from a psychology perspective and presents guidelines for digital representations \cite{Johnson1998}.\par
Cuddihy and Walters defined an early model for assessing desktop interaction mechanisms for social virtual environments \cite{Cuddihy2000}.                               
\subsubsection{Perception Of Honesty}
Hancock et al. state that we are most likely to lie, and to be lied to, on the telephone \cite{Hancock2004}. Technology used for communication impacts interpersonal honesty. It seems that at some level humans know this; lack of eye contact leads to feelings of deception, impacting trust \cite{Holm2010}. This has a major impact on video conferencing which often does not support mutual gaze. Trust is crucial for business interactions.\par
Further there are universal expressions, micro-expressions, and blink rate which can betray hidden emotions \cite{porter2008reading}, though the effects are subtle and there is a general lack of awareness by humans of their abilities in this regard \cite{Holm2010}. Absence of support for such instinctive cues inhibits trust \cite{Roberts2015}. Support for these rapid and transient facial features demands high resolution reproduction in both resolution and time domains. There is detectable difference in a participant's ability to detect deception when between video conference mediated communication and that mediated by avatars \cite{Steptoe2010b}. Systems should aim for maximally faithful reproduction. 
%\section{Technology-Mediated Interaction}
\subsection{Dyadic}
\subsubsection{Point to Point Telephony (Audio only)}
The ubiquitous technology to mediate conversation is, of course, the telephone. The 2015 Ericsson mobility report \cite{2015} states that there are a staggering 7.4 billion mobile subscriptions. More people have access to mobile phones than to working toilets. %\cite{UNAccesstoToilets}.\par
Joupii and Pan designed a system which focused attention on spatially correct high definition audio. They found ``significant improvement over traditional audio conferencing technology, primarily due to the increased dynamic range and directionality. \cite{jouppi2002mutually}. Aoki et al. also describe an audio only system with support for spatial cues \cite{Aoki2003}.
            \subsubsection{Point to Point Video Conferencing}
                O'Malley et al. showed that face-to-face and video mediated employed  visual cues for mutual understanding, and that addition of video to the audio channel aided confidence and mutual understanding. However, video mediated did not provide the clear cues of being co-located \cite{O'Malley1996}.\par
                Dourish et al. make a case for not using face to face as a baseline for comparison, but rather that analysis of the efficacy of remote tele-collaboration tools should be made in a wider context of connected multimedia tools and `emergent communicative practises' \cite{Dourish1993}. While this is an interesting viewpoint it does not necessarily map well to a recreation of the ad-hoc meeting.\par
There is established literature on human sensitivity to eye contact in both 2D and 3D VC \cite{Criminisi2003, Eijk2010}, with an accepted minimum of 5-10 degreesbefore observers can reliably sense they are not being looked at \cite{Chen2002}. Roberts et al. suggested that at the limit of social gaze distance (~4m) the maximum angular separation between people standing shoulder to shoulder in the real world would be around 4 degrees\cite{Roberts2013}. \par
                Sellen found limited impact on turn passing when adding a visual channel to audio between two people when using Hydra, an early system which provided multiple video conference displays in an intuitive spatial distribution\cite{Sellen1992a}. She did however, find that the design of the video system affected the ability to hold multi-party conversations \cite{Sellen1995a}.\par
                Monk and Gale describe in detail experiments which they used 
 for examining gaze awareness in communication which is mediated and unmediated by technology.  
  They   found that gaze awareness increased message understanding  
  \cite{Monk2002}.\par
                Both Kuster et al. and Gemmel et al. have successfuly demonstrated software systems which can adjust eye gaze to correct for off axis capture in real time video systems\cite{Gemmell2000, Kuster2012b}.\par
                Shahid et al. conducted a study on pairs of children playing games with and without video mediation and concluded that the availability of mutual gaze affordance enriched social presence and fun, while its absence dramatically affects the quality of the interaction. They used the `Networked Minds', a social presence questionnaire.                
        \subsection{Triadic and Small Group}
Early enthusiasm in the 1970's for video conferencing, as a medium for small group interaction quickly turned to disillusionment. It was agreed after a flurry of initial research that the systems at the time offered no particular advantage over audio only communication, and at considerable cost \cite{Williams1977}.\par
Something in the breakdown of normal visual cues seems to impact the ability of the technology to support flowing group interaction. Nonetheless, some non-verbal communication is supported in VC with limited success. \par
Additional screens and cameras can partially overcome the limitation of no multi-party support (that of addressing a room full of people on a single screen) by making available more bidirectional channels. For instance, every remote user can be a head on a screen with a corresponding camera. The positioning of the screens must then necessarily match the physical organization of the remote room.\par
Egido provides an early review of the failure of VC for group activity, with the ``misrepresentation of the technology as a substitute for face to face" still being valid today \cite{Edigo1988}.\par
Commercial systems such as Cisco Telepresence Rooms cluster their cameras above the centre screen of three for meetings using their telecollaboration product, while admitting that this only works well for the central seat of the three screens. They also group multiple people on a single screen in what Workhoven et al. dub a ``non-isotropic" configuration \cite{Pejsa2016}. They maintain that this is a suitable trade off as the focus of the meeting is more generally toward the important contributor in the central seat \cite{CiscoSystems}. This does not necessarily follow for less formal meeting paradigms.\par
            In small groups, it is more difficult to align non-verbal cues between all  parties, and at the same time, it is more important because the hand-offs between parties are more numerous and important in groups. A breakdown in conversational flow in such circumstances is harder to solve. A perception of the next person to talk must be resolved for all parties and agreed upon to some extent.\par
                However, most of the conventional single camera, and expensive multi camera VC systems, suffer a fundamental limitation in that the offset between the camera sight lines and the lines of actual sight introduce incongruities that the brain must compensate for \cite{Wolff2008}.\par
   %  For experimental design Bailenson found the game `20 questions' to be effective in analysis of triadic attention, specifically watching the head gaze \cite{Bailenson2002}.
        \subsection{Presence, Co-presence, and Social Presence}
            Presence is a heavily cited historic indicator of engagement in virtual reality, though the precise meaning has been interpreted differently by different specialisms \cite{Beck2011, Schuemie2001}. It is generally agreed to be the 'sense of being' in a virtual environment \cite{Slater1999}. Slater extends this to include the ``extent to which the VE becomes dominant". \par
Beck et al. reviewed 108 articles and synthesised an ontology of presence \cite{Beck2011} which at its simplest is as follows:
            \begin{enumerate}
				\item Sentient presence
                    \begin{enumerate}
                     \item Physical interaction
                      \item Mental interaction
                    \end{enumerate}
                   \item Non-sentient
                   \begin{enumerate}
                       \item Physical immersion
                       \item Mental immersion = psychological state
                     \end{enumerate}
            \end{enumerate}
            
When presence is applied to interaction it may be split into Telepresence, and Co/Social presence  \cite{heeter1992being, Biocca1997}.  Co-presence and/or social presence is the sense of ``being there with another", and describes the automatic responses to complex social cues \cite{doi:10.1080/01449299508914633, fulk1987social, haythornthwaite1995work}.    Social presence (and co-presence) refers in this research context to social presence which is mediated by technology (even extending to text based chat \cite{Gunawardena1997}), and has its foundations in psychological mechanisms which engender mutualism in the `real'. This is analysed in depth by Nowak \cite{Nowak2001}. An examination of telepresence, co-presence and social presence necessarily revisits some of the knowledge already elaborated.\par
        The boundaries between the three are blurred in research with conflicting results presented \cite{Bulu2012}. Biocca et al. attempted to enumerate the different levels and interpretations surrounding these vague words \cite{Biocca2003}, and to distill them into a more robust theory which better lends itself to measurement. They suggest a solid understanding of the surrounding psychological requirements which need support in a mediated setting, and then a scope that is detailed and limited to the mediated situation.\par
 Since `social presence' has been subject to varied definitions \cite{Biocca2003} it is useful here to consider a single definition from the literature which defines it as ``the ability of participants in the community of inquiry to project their personal characteristics into the community, thereby presenting themselves to the other participants as real people.'' \cite{Garrison1999, Beck2011}. Similarly to specifically define co-presence for this research it is taken to be the degree to which participants in a virtual environment are ``accesible, available, and subject to one another" \cite{Biocca2003}. \par
            Social presence has received much attention and there are established questionnaires used in the field for measurement of the levels of perceived social presence yet the definitions here also remain broad, with some confusion about what is being measured \cite{Biocca2003a}.\par            
 Telepresence meanwhile is interaction with a different (usually remote) environment which may or may not be virtual, and may or may not contain a separate social/co-presence component. \par 
       Even in simple videoconferencing Bondareva and Bouwhuis stated (as part of an experimental design) that the following determinants are important to create social presence \cite{Bondareva2004, jouppi2002mutually}. 
            \begin{enumerate}
            \item    Direct eye contact is preserved
            \item    Wide visual field
            \item    Both remote participants appear life size
            \item    Possibility for participants to see the upper body of the interlocutor
            \item    High quality image and correct colour reproduction
            \item    Audio with high S/N ratio
            \item    Directional sound field
            \item    Minimization of the video and audio signal asynchrony
            \item    Availability of a shared working space.
            \end{enumerate}
			
			     
            Bondareva et al. went on to describe a person-to-person telepresence system with a semi-silvered mirror to reconnect eye gaze, which they claimed increased social presence indicators. Interestingly they chose a checklist of interpersonal interactions which they used against recordings of conversations through the system \cite{Bondareva2004}.  \par
            The idea of social presence as an indicator of the efficacy of the system, suggests the use of social presence questionnaires in the evaluation of the system \cite{Biocca2003}.  Subjective questionnaires are however troublesome in measuring effectiveness of virtual agents and embodiments, with even nonsensical questions producing seemingly valid results \cite{Slater2004}. Usoh et al. found that 'the real' produced only marginally higher presence results than the virtual \cite{Usoh2000a}.\par
            Nowak states that ``A satisfactory level of co-presence with another mind can be achieved with conscious awareness that the interaction is mediated" and asserts that while the mediation may influence the degree of co-presence it is not a prohibiting factor \cite{Nowak2001}.\par 
            Baren and IJsselsteijn \cite{Baren, Harms2004} list 20 useful presence questionnaires in 2004 of which ``Networked Minds" seemed most appropriate for the research.
            Hauber et al. employed the ``Networked Minds" Social Presence questionnaire experimentally and found that while the measure could successfully discriminate between triadic conversation that is mediated or unmediated by technology, it could not find a difference between 2D and 3D mediated interfaces \cite{Hauber2005, Gunawardena1997}.\par
			In summary, social presence and co-presence are important historic measures of the efficacy of a communication system. Use of the term in literature peaked between 1999 and 2006 according to Google's ngram viewer and has been slowly falling off since. The questionnaire methodology has been challenged in recent research and while more objective measurement may be appropriate, the networked minds questions seem to be able to differentiate real from virtual interactions \cite{Harms2004}.
\subsection{Other Systems to Support Business}                  
There have been many attempts to support group working and rich data sharing between dispersed groups in a business setting. So called 'smart spaces' allow interaction with different displays for different activities and add in some ability to communicate with remote or even mobile collaborators on shared documents \cite{Bardram2012}, with additional challenges for multi-disciplinary groups who are perhaps less familiar with one or more of the technology barriers involved \cite{Adamczyk2007}.\par
Early systems like clearboard \cite{Ishii1993} demonstrated the potential for smart whiteboards with a webcam component for peer to peer collaborative working. Indeed it is possible to support this modality with Skype and a smartboard system (and up to deployments such as Accessgrid). They remain relatively unpopular however.\par

                   \subsection{Mutual Gaze in Telepresence}
          We have seen that transmission of attention can broadly impact communication in subtle ways, impacting empathy, trust, cognition, and co-working patterns. Mutual gaze (looking into one another's eyes), is currently the high water mark for technology-mediated conversation.\par
          Many attempts have been made to re-unite mutual eye gaze when using tele-conferencing systems. In their 2015 review of approaches Regenbrecht and Langlotz found that none of the methods they examined were completely ideal \cite{Regenbrecht2015}. They found most promise in 2D and 3D interpolation techniques, which will be discussed in detail later, but they opined that such systems were very much ongoing research and lacked sufficient optimisation.\par
          A popular approach uses the so called 'Peppers Ghost' phenomenon \cite{steinmeyer2013science}, where a semi silvered mirror presents an image to the eye of the observer, but allows a camera to view through from behind the angled mirror surface. The earliest example of this is Rosental's two way television system in 1947 \cite{rosenthal1947two}, though Buxton et al. `Reciprocal Video Tunnel' from 1992 is more often cited \cite{buxton1992telepresence}. This optical characteristic isn't supported by retroreflective projection technology, and besides requires careful control of light levels either side of the semi-silvered surface.\par  
The early GAZE-2 system (which makes use of Pepper's ghost) is novel in that it uses an eye tracker to select the correct camera from several trained on the remote user. This ensures that the correct returned gaze (within the ability of the system) is returned to the correct user on the other end of the network \cite{Vertegaal2003}.
Mutual gaze capability is later highlighted as an affordance supported or unsupported by key research and commercial systems.                           
\subsection{Other Systems to Support Business}                  
There have been many attempts to support group working and rich data sharing between dispersed groups in a business setting. So called 'smart spaces' allow interaction with different displays for different activities and add in some ability to communicate with remote or even mobile collaborators on shared documents \cite{Bardram2012}, with additional challenges for multi-disciplinary groups who are perhaps less familiar with one or more of the technology barriers involved \cite{Adamczyk2007}.\par
Early systems like clearboard \cite{Ishii1993} demonstrated the potential for smart whiteboards with a webcam component for peer-to-peer collaborative working. Indeed it is possible to support this modality with Skype and a smartboard system (and up to deployments such as Accessgrid). They remain relatively unpopular however.\par
\subsubsection{Ad-hoc in SecondLife, immersive VR}
something about the bitcoin VR meetups and other ad-hoc groupings that are emerging.
  \subsubsection{Tabletop and Shared Task}
In early telepresence research Buxton and William argued through examples that ``effective telepresence depends on quality sharing of both person and task space \cite{Buxton1992a}.\par
In their triadic shared virtual workspace Tang et al. found difficulty in reading shared text using a `round the table' configuration, a marked preference for working collaboratively on the same side of the table. They also found additional confusion as to the identity of remote participants \cite{Tang2010}.
Tse et al. found that pairs can work well over a shared digital tabletop, successfully overcoming a single user interface to interleave tasks \cite{Tse2007}.\par
Tang et al. demonstrate that collaborators engage and disengage around a group activity through several distinct, recognizable mechanisms with unique characteristics \cite{Tang2006}. They state that tabletop interfaces should offer a variety of tools to facilitate this fluidity.\par
Camblend is a shared workspace with panoramic high resolution video. It maintains some spatial cues between locations by keeping a shared object in the video feeds \cite{Norris2013a, Norris2012}. Participants successfully resolved co-orientation within the system.\par
The t-room system implemented by Luff et al. surrounds co-located participants standing at a shared digital table with life sized body and head video representations of remote collaborators \cite{Luff2011} but found that there were incongruities in the spatial and temporal matching between the collaborators which broke the flow of conversation.
Tuddenham et al. found that co-located collaborators naturally devolved 'territory' of working when sharing a task space, and that this did not happen the same way with a tele-present collaborator \cite{tuddenham2009territorial}. Instead remote collaboration adapted to use a patchwork of ownership of a shared task. It seems obvious to say that task ownership is a function of working space, but it is interesting that the research found no measurable difference in performance when the patchwork coping strategy was employed.\par
The nature of a shared collaborative task and/or interface directly impacts the style of interaction between collaborators. This will have a bearing on the choice of task for experimentation \cite{Jamil2011, Jetter2011a}.


\subsection{Beyond 2D Screens}
            Displays need not be limited to 2 dimensional screens and can be enhanced in various ways.\par
                Stereoscopy allows an illusion of depth to be added to a 2D image by exploiting the stereo depth processing characteristics of the human vision system. This technical approach is not perfect as it does not fully recreate the convergence and focus expected by the eyes and brain.\par %\cite{illusionStereoscopy}. 
                There are multiple approaches to separating the left and right eye images, these primarily being active (where a signal selectively blanks the input to left then right eyes in synchronicity with the display), passive, where either selective spectrum or selective polarisation of light allow different portions of a display access to different eyes, or physical arrangements which present different displays (or slices of light as in lenticular systems) to different eyes.\par
                These barrier stereoscopy / lenticular displays use vertical light barriers built into the display to create multiple discrete channels of display which are accessed by moving horizontally with respect to the display. In this way it is possible to generate either a left/right eye image pair for 'autostereoscopic' viewing, or with the addition of head tracking and small motors. With these techniques multiple viewpoint or an adaptive realtime viewpoint update can be presented without the glasses required for active or passive stereoscopic systems. \par
            \subsubsection{Spatially Faithful Group}
                Hauber et al. combined videoconferencing, tabletop, and social presence analysis and tested the addition of 3D. They found a nuanced response when comparing 2D and 3D approaches to spatiality: 3D showed improved presence over 2D (chiefly through gaze support), while 2D demonstrated improved task performance because of task focus \cite{Hauber2006}.\par
                I3DVC reconstructs participants from multiple cameras and places them isotropically (spatially faithful) \cite{Kauff2002, Kauff2002a}. The system uses a large projection screen, a custom table, and carefully defined seating positions. They discussed an ``extended perception space" which used identical equipment in the remote spaces in a tightly coupled collaborative `booth'. It employed head tracking and multi camera reconstruction alongside large screens built into the booth. This system exemplified the physical restrictions which are required to limit the problems of looking into another space through the screen. Fuchs et al. demonstrated a similar system over a wide area network but achieved only limited resolution and frame rate with the technology of the day \cite{Fuchs:2002ww}. \par University of Southern California used a technically demanding real-time set-up with 3D face scanning and an autostereoscopic 3D display to generate multiple `face tracked' viewpoints \cite{jones2009headspin}. This had the disadvantage of displaying a disembodied head.\par                
MAJIC is an early comparable system to support small groups with life size spatially correct video, but without multiple viewpoints onto the remote collaborators it was a one to 'some' system rather than 'some' to one. Additionally users were rooted to defined locations \cite{Ichikawa1995, Okada:1994et}.\par


\textbf{Multiview}
In order to reconnect directional cues of all kinds it is necessary for each party in the group to have a spatially correct view of the remote user which is particular for them. This requires a multi-view display, which has applications beyond telepresence but are used extensively in research which attempts to address these issues.\par
Nguyen and Canny demonstrated the `Multiview' system \cite{Nguyen2005}. Multiview is a spatially segmented system, that is, it presents different views to people standing in different locations simultaneously. They found similar task performance in trust tasks to face to face meetings, while a similar approach without spatial segmentation was seen to negatively impact performance.\par
                    
%ref Multi-View Lenticular Display for Group Teleconferencing \cite{Lincoln2009a}
In addition to spatial segmentation of viewpoints \cite{gotsch2018telehuman2} it is possible to isolate viewpoints in the time domain. Different tracked users can be presented with their individual view of a virtual scene for a few milliseconds per eye, before another viewpoint is shown to another user. Up to six such viewpoints are supported in the c1x6 system \cite{Kulik2011}
Similarly MM+Space offered 4 Degree-Of-Freedom Kinetic Display to recreate Multiparty Conversation Spaces \cite{Otsuka2013}
            \subsubsection{Robots, Shader Lamp, and Hybrid}
                Virtuality human representation extends beyond simple displays into robotic embodiments (which need not be humanoid \cite{Marti2005}), shape mapped projection dubbed ``shader lamps", and hybridisations of the two.\par 
                
				\textbf{Uncanniness}
				
When employing simulation representations of humans it may be the case that there is an element of weirdness to some of these systems, especially those that currently represent a head without a body. Mori has demonstrated The Uncanny Valley \cite{Mori1970} effect in which imperfect representations of humans elicit revulsion in certain observers. This provides a toolkit for inspecting potentially `weird' representations, especially if they are `eerie' and is testable through Mori's GODSPEED questionnaire. \par
                    With an improved analysis of the shape of the likeability curve estimated later showing a more nuanced response from respondents where anthropomorphism of characters demonstrated increased likeability even against a human baseline \cite{Bartneck2007, Bartneck2009a}.\par
                    A mismatch in the human realism of face and voice also produces an Uncanny Valley response \cite{Mitchell2011}.\par
                    However, there is a possibility that Mori's hypothesis may be too simplistic for practical everyday use in CG and robotics research since anthropomorphism can be ascribed to many and interdependent features such as movement and content of interaction \cite{Bartneck2009}.\par
                    Bartneck et al. also performed tests which suggest that the original Uncanny Valley assertions may be incorrect, and that it may be inappropriate to map human responses to human simulacrum to such a simplistic scale. They suggest that the measure has been a convenient `escape route' for researchers \cite{Bartneck2009}. Their suggestion that the measure should not hold back the development of more realistic robots holds less bearing for the main thrust of this telepresence research which seeks to capture issues with imperfect video representation rather than test the validity of an approximation.\par
                    Interestingly Ho et al. performed tests on a variety of facial representations using images.  They found that facial performance is a `double edged sword' with realism being important to robotic representations, but there also being a significant Uncanny Valley effect around `eerie, creepy, and strange' which can be avoided by good design \cite{Ho2008}.\par
                    More humanlike representations exhibiting higher realism produce more positive social interactions when subjective measures are used \cite{Yee2007a} but not when objective measures are used. This suggests that questionnaires may be more important when assessing potential uncanniness.\par
                    Ho et al. also identified problems with the original GODSPEED indices used to measure Uncanny Valley effects. They proposed a new set of measures which they found to be generally valid \cite{Ho:2010:RUV:1853385.1853509}, though they admit they were only tested with a single set of stimuli.\par
                    A far more objective method would be to measure user responses to humans, robots, and representations with functional near-infrared spectroscopy and while this has been attempted it is early exploratory research \cite{Strait2014}, an emotional response to `eerie' was discovered.\par
                    
\textbf{Embodiment through robots}

                    Robots which carry a videoconference style screen showing a head can add mobility and this extends the available cues \cite{Adalgeirsson2010, Lee2011b, Tsui2011, Paulos1998, Kristoffersson2013}. Interestingly Desai and Uhlik maintain that the overriding modality should be high quality audio \cite{Desai2011}.\par
                    Tsui et al. asked 96 participants to rate how personal and interactive they found interfaces to be. Interestingly they rated videoconferencing as both more personal and more interactive than telepresence robots, suggesting that there is a problem with the overall representation or embodiment \cite{Tsui2012}.\par
                    Kristoffersson et al. applied the Networked Minds questionnaire to judge presence of a telepresence robot for participants with little or no experience of videoconferencing. Their results were encouraging, though they identified that the acuity of the audio channel needing improvement \cite{Kristoffersson2011}.\par
                    There are a very few lifelike robots which can be used for telepresence, and even these are judged to be uncanny \cite{Sakamoto2007}. This is only an issue for a human likeness since anthropomorphic proxies such as robots and toys perform well \cite{Mori1970}.\par
                    
 \textbf{Physical \& Hybrid embodiment}
 
                    Embodiment through hybridisation of real-time video and physical animatronic mannequins has been investigated as a way to bring the remote person into the space in a more convincing way \cite{Lincoln2009, Lincoln:2010it, Raskar2001a}. \ These include telepresence robots \cite{Lee2011b, Sakamoto2007, Tsui2011}, head in a jar implementations such as SphereAvatar \cite{Oyekoya2012, pan2014comparing, Pan2012a} and BiReality \cite{Jouppi2004}, \ UCL's Gaze Preserving Situated Multi-View Telepresence System \cite{Pan2014a}, or screen on a stick style representations \cite{Kristoffersson2013}.\par  
                    Nagendran et al. present a 3D continuum of these systems into which they suggest all such systems can be rated from artificial to real on the three axes, shape, intelligence, and appearance \cite{Nagendran}.\par
                    Itoh et al. describe a 'face robot' to convey captured human emotion over a distance. It uses an `average face' and actuators to manipulate feature points \cite{Itoh2005}. It seems that this is an outlier method for communication of facial affect but demonstrates that there are many development paths to a more tangible human display.\par 
                    
\textbf{Shader lamps}

Projection mapping is a computational augmented projection technique where consideration of the relative positions and angles of complex surfaces allows the projection from single or multiple sources to augment the physical shapes onto which they appear. It was first considered by the Disney corporation in 1969 \cite{projectionmappingweb2013} and was given prominence by Raskar and Fuchs with ``office of the future" \cite{Raskar1998} and later by Raskar and other researchers \cite{Low2001, Raskar2001a}. It has since gained considerable commercial popularity in live entertainment \cite{googleStatsProjectionMapping}.\par
                    Shader lamps \cite{Raskar2001a} is the more formal academic designation for projection mapping. It is possible to use the technique alongside reconstruction to project onto a white facial mannequin. Researchers have attempted to use the technology for remote patient diagnostic, projecting onto styrofoam heads  \cite{Rivera-Gutierrez2012}.\par          
                     Bandyopadhyay et al. demonstrated \cite{Bandyopadhyay} that it is possible to track objects and projection map \cite{Dalsgaard2011} onto them in real time. This is beyond the scope of the proposed projection onto furniture since we wish to keep the system as simple as possible, but could be useful for shared tasks in the future work.\par
                    Lincoln et al. employed animatronic avatars which they projected with shader lamps. This combination recreated facial expression and head movement though they were limited in speed and range of control of the remote head \cite{Lincoln:2010it}.\par
                    While shader lamps are an important and useful technology, there are limitations imposed by its use. In particular if a realtime video feed or reconstruction of a subject is used then that scanned subject must either remain still enough to be correctly mapped onto geometry on the remote side (useful for some virtual patients for instance \cite{Benjamin2012}, or else there must be a computational adjustment made for their changing position to make them appear static, or the projection surface must move to match their movement as in Lincoln et al. .
   
\subsubsection{Holography and Volumetric}
Blanche et al. have done a great deal of research into holographic and volumetric displays using lasers, rotating surfaces, and light field technology   \cite{Blanche2010,tay2008updatable}. They are actively seeking to use their technologies for telepresence and their work is very interesting.\par
Similarly Jones et al. ``HeadSPIN" is a one-to-many 3D video teleconferencing system \cite{jones2009headspin} which uses a rotating display to render the holographic head of a remote party. They achieve transmissible and usable framerate using structured light scanning of a remote collaborator as they view a 2D screen which they say shows a spatially correct view of the onlooking parties.\par
Eldes et al. used a rotating display to present multi-view autostereoscopic projected images to users \cite{eldes2013multi}.\par
Seelinder is an interesting system which uses parallax barriers to render a head which an onlooking viewer can walk around. The system uses 360 high resolution still images which means a new spatially segmented view of the head every 1 degreesof arc. They claim the system is capable of playback of video and this head in a jar multi-view system clearly has merit but is comparatively small, and as yet untested for telepresence \cite{Yendo2010}.\par
These systems do not satisfy the requirement to render upper body for the viewers and are not situated (as described soon).\par

 
\subsubsection{Reconstructed Viewpoint}
Although it is possible to manipulate a 2D view of a remote participant to bring the camera, eye, and screen into correct alignment better results can be obtained by capturing a 3D representation of the remote user, then modifying that. Several groups have investigated stereo reconstruction of face and upper body, so-called immersive videoconferencing \cite{Atzpadin2004}. \par                   Multiple viewpoints from multiple cameras allow either playback from different cameras on a continuous basis, so-called free viewpoint TV \cite{Zhang2007}. This multiple camera approach can be extended with the use of light field to interpolate between real or virtual cameras \cite{Ott1993, Al-Saidi2009}, or else algorithmic generation and display of a polygonal mesh through photogrammetry can be undertaken. This can now be performed fast enough to compute a continuous surface from a video stream with little latency \cite{Criminisi:2003ji}. 
Engineers from Google presented an efficient method for network transmission of light field scenes at Siggraph \cite{broxton2020immersive}.
These multi camera systems aim to create geometry for a single virtual viewpoint (such as a face in video conferencing), which is correct to align eye gaze. This is sometimes called ``3D video", which is different to stereoscopic video used in cinema and commercial TV.\par
 A depth map, point cloud, then polygon mesh can be calculated for one side of a person from two adjacent cameras which have slightly different views of a subject.  This so-called `narrow baseline' technique examines sub-pixel disparities between the images \cite{Knoblauch2008}. \par

Multiple `narrow baseline' pairs can create stereo geometry reliably from a tightly coupled pair of cameras which are integrated with geometry from another pair with an independent viewpoint \cite{Okutomi:1993ez, Cooke2002, Cooke2002a}. \par
Alternatively there is the so-called ``wide baseline" approach in which a pair of cameras with a significant distance between them capture both sides of the face or body. Sadagic et al. demonstrated a system which brought 2 camera reconstructed \cite{Sadagic2001} people into a desk based collaborative space. The user addressing these two tele-present collaborators could move around in their chair while being presented with both passive stereo depth cues, and parallax cues appropriate to the desk environment. The system's main limitation was the necessity for the user who addressed the two desk screens to wear both stereo glasses and a camera on their head. This made their system unbalanced.  There are also problems with geometry and texture management using a wide baseline approach since the cameras are resolving for different lighting and views of the scene.\par
      Pan, Steptoe and Steed found similar results with their spherical display with a decrease in trust toward avatar mediated conversation when viewing 2D displays at oblique angles \cite{pan2014comparing}.\par
Microsoft Kinect sensors demonstrate effectiveness in real-time scanning with between 1 and 5 deployed in research systems \cite{Mekuria2013}. Multiple Kinect v1 sensors interfere with one another, and though this problem is partially addressed with v2 hardware there are constraints with temperature drift causing frequency shifts which allow interference to creep in. There are workarounds for this issue but no completely reliable implementation.\par

                For some time it has been possible to create geometric models of the human form using a technique called ``shape from silhouette" \cite{Allard2006, Petit:2008tva,Matusik2000,Starck2008,Franco2003,Baumgart1975,Laurentini1994a,Grau2007,Waizenegger2011,Franco2009,Feldmann2009,Cooke2000, Starck2003}\par
                Similar effect can also be accomplished for the human shape using depth cameras such as the Microsoft Kinect v2 \cite{Maimone2011}, and indeed this seems now to be the prevailing method. Sensors in this vein continue to improve.\par
       Structured light systems provide still another technique, back as early as the 1980's \cite{Hu:1986ww}, and as recently as the Microsoft Kinect v1 sensor. Deformations in projected visible or infrared patterns can be compared to the known baseline and shape reverse engineered from the differences. 3D-Board is an interesting example since two standing collaborators can interact through touch with a digital whiteboard which also forms the perceived barrier between them \cite{Zillner2014}.\par
       \textbf{Rendering and telepresence}
       
                        Cooke et al. investigate the difference between so-called narrow and wide baseline camera capture for dealing with the more complicated elements of reconstructed geometry such as hands which gesture. They maintain that multiple stereo camera pairs (each narrow baseline) arranged as a wide baseline system, with post processing to remove artifacts is of most use \cite{Cooke_image-basedrendering}.\par
                        Petit et al. adopt a wide baseline system with conventional green screen background segmentation and get good 3D video results. Because they are using a chroma based technique for their silhouette system they cannot exclude objects in the scene by `training' the system \cite{Petit2010}.\par
Kuster et al. use a depth based approach and have a compelling demonstrator which uses what they call an ``anisotropic transparent background back projection foil''. They rear project a Kinect reconstructed head and upper body in stereoscopic 3D onto a transparent film suspended so as to appear to bring the remote person to the edge of a table. This brings the 3D avatar into a space at 15 frames per second across significant network distances but is again peer to peer\cite{Kuster2012a}.\par
Similarly Maimone and Fuchs use large tiled displays and Kinect sensors but attempt to 'merge' two spaces by scanning both the person and the remote space to create perspective correct viewing through a virtual window into the other space. Without multi-view this system is one to one, although they demonstrate multiple people in the background of the space \cite{Maimone2011a}.\par
       The Fusion4D system successfully captured complex and challenging scenes such as dancers with flowing clothing for playback from arbitrary angles \cite{Dou2016}.\par
withyou is an experimental capture and playback system which uses the Octave multi-modal suite. This system uses wide baseline single cameras and CPU based shape from silhouette reconstruction alongside a novel network transport system to send a full 3D video polygonal hull to another rendering location \cite{Roberts2015}. Previous tests on the system suggested that the capture and playback made it possible to judge the eye gaze of the reconstructed subjects to within normal social/biological limits \cite{Roberts2013}.\par
A particularly well developed example is the blue-c system which enables telepresence through multi-camera 3D video. The users at multiple locations are scanned and transmitted as a point cloud captured during projector blanking frames \cite{Gross2003}. The system is flexible enough to use either CAVE style surround or a projected screen style arrangement.\par
The ViPiD system interestingly uses Chromatte in its intended role to improve segmentation for their capture system \cite{Klie2006}. \par
Schreer at al demonstrate a capable multi camera capture system suitable for small group telepresence, though its technical overhead is high \cite{Schreer:2008ty}. They discuss the need for multi-view capable screens only insomuch as they agree one must be developed for the system.\par
Maimone at al demonstrate a system uses a HMD system to project a reconstructed person into the correct seat at a table in augmented reality \cite{Maimone2013}.\par
                        
%{matusik2000image,     title={Image-based visual hulls},     author={Matusik, Wojciech and Buehler, Chris and Raskar, Ramesh and   Gortler, Steven J and McMillan, Leonard},     booktitle={Proceedings of the 27th annual conference on Computer   graphics and interactive techniques},     pages={369--374},     year={2000},     organization={ACM Press/Addison-Wesley Publishing Co.}

%http://gl.ict.usc.edu/Research/PicoArray/
%http://gl.ict.usc.edu/Research/CNNHologram/
            


A depth map, point cloud, then polygon mesh can be calculated for one side of a person from two adjacent cameras which have slightly different views of a subject.  This so-called `narrow baseline' technique examines sub-pixel disparities between the images \cite{Knoblauch2008}. \par
Multiple `narrow baseline' pairs can create stereo geometry reliably from a tightly coupled pair of cameras which are integrated with geometry from another pair with an independent viewpoint \cite{Okutomi:1993ez, Cooke2002, Cooke2002a}. \par
Alternatively there is the so-called ``wide baseline" approach in which a pair of cameras with a significant distance between them capture both sides of the face or body. Sadagic et al. demonstrated a system which brought 2 camera reconstructed \cite{Sadagic2001} people into a desk based collaborative space. The user addressing these two tele-present collaborators could move around in their chair while being presented with both passive stereo depth cues, and parallax cues appropriate to the desk environment. The system's main limitation was the necessity for the user who addressed the two desk screens to wear both stereo glasses and a camera on their head. This made their system unbalanced.  There are also problems with geometry and texture management using a wide baseline approach since the cameras are resolving for different lighting and views of the scene.\par
      Pan, Steptoe and Steed found similar results with their spherical display with a decrease in trust toward avatar mediated conversation when viewing 2D displays at oblique angles \cite{pan2014comparing}.\par
Microsoft Kinect sensors demonstrate effectiveness in real-time scanning with between 1 and 5 deployed in research systems \cite{Mekuria2013}. Multiple Kinect v1 sensors interfere with one another, and though this problem is partially addressed with v2 hardware there are constraints with temperature drift causing frequency shifts which allow interference to creep in. There are workarounds for this issue but no completely reliable implementation.\par

                For some time it has been possible to create geometric models of the human form using a technique called ``shape from silhouette" \cite{Allard2006, Petit:2008tva,Matusik2000,Starck2008,Franco2003,Baumgart1975,Laurentini1994a,Grau2007,Waizenegger2011,Franco2009,Feldmann2009,Cooke2000, Starck2003}\par
                Similar effect can also be accomplished for the human shape using depth cameras such as the Microsoft Kinect v2 \cite{Maimone2011}, and indeed this seems now to be the prevailing method. Sensors in this vein continue to improve.\par
       Structured light systems provide still another technique, back as early as the 1980's \cite{Hu:1986ww}, and as recently as the Microsoft Kinect v1 sensor. Deformations in projected visible or infrared patterns can be compared to the known baseline and shape reverse engineered from the differences. 3D-Board is an interesting example since two standing collaborators can interact through touch with a digital whiteboard which also forms the perceived barrier between them \cite{Zillner2014}.\par
       \textbf{Rendering and telepresence}
       
    
                        Cooke et al. investigate the difference between so-called narrow and wide baseline camera capture for dealing with the more complicated elements of reconstructed geometry such as hands which gesture. They maintain that multiple stereo camera pairs (each narrow baseline) arranged as a wide baseline system, with post processing to remove artifacts is of most use \cite{Cooke_image-basedrendering}.\par
                        Petit et al. adopt a wide baseline system with conventional green screen background segmentation and get good 3D video results. Because they are using a chroma based technique for their silhouette system they cannot exclude objects in the scene by `training' the system \cite{Petit2010}.\par
Kuster et al. use a depth based approach and have a compelling demonstrator which uses what they call an ``anisotropic transparent background back projection foil''. They rear project a Kinect reconstructed head and upper body in stereoscopic 3D onto a transparent film suspended so as to appear to bring the remote person to the edge of a table. This brings the 3D avatar into a space at 15 frames per second across significant network distances but is again peer-to-peer\cite{Kuster2012a}.\par
Similarly Maimone and Fuchs use large tiled displays and Kinect sensors but attempt to 'merge' two spaces by scanning both the person and the remote space to create perspective correct viewing through a virtual window into the other space. Without multi-view this system is one to one, although they demonstrate multiple people in the background of the space \cite{Maimone2011a}.\par
       The Fusion4D system successfully captured complex and challenging scenes such as dancers with flowing clothing for playback from arbitrary angles \cite{Dou2016}.\par
withyou is an experimental capture and playback system which uses the Octave multi-modal suite. This system uses wide baseline single cameras and CPU based shape from silhouette reconstruction alongside a novel network transport system to send a full 3D video polygonal hull to another rendering location \cite{Roberts2015}. Previous tests on the system suggested that the capture and playback made it possible to judge the eye gaze of the reconstructed subjects to within normal social/biological limits \cite{Roberts2013}.\par
A particularly well developed example is the blue-c system which enables telepresence through multi-camera 3D video. The users at multiple locations are scanned and transmitted as a point cloud captured during projector blanking frames \cite{Gross2003}. The system is flexible enough to use either CAVE style surround or a projected screen style arrangement.\par
The ViPiD system interestingly uses Chromatte in its intended role to improve segmentation for their capture system \cite{Klie2006}. \par
Schreer at al demonstrate a capable multi camera capture system suitable for small group telepresence, though its technical overhead is high \cite{Schreer:2008ty}. They discuss the need for multi-view capable screens only insomuch as they agree one must be developed for the system.\par
Maimone at al demonstrate a system uses a HMD system to project a reconstructed person into the correct seat at a table in augmented reality \cite{Maimone2013}.\par
                        
%{matusik2000image,     title={Image-based visual hulls},     author={Matusik, Wojciech and Buehler, Chris and Raskar, Ramesh and   Gortler, Steven J and McMillan, Leonard},     booktitle={Proceedings of the 27th annual conference on Computer   graphics and interactive techniques},     pages={369--374},     year={2000},     organization={ACM Press/Addison-Wesley Publishing Co.}

%http://gl.ict.usc.edu/Research/PicoArray/
%http://gl.ict.usc.edu/Research/CNNHologram/

        \subsubsection{Immersive Collaborative Virtual Environments}
Simple online virtual environments with an external viewpoint and many avatars interacting online generated much hype in the early days as a potential means to support group meetings, even back as far as the VRML standard \cite{Ferscha1999}. SecondLife in particular generated significant interest. Erickson et al. went so far as to say that a virtual conference hosted in SecondLife was 'fairly successful' \cite{Erickson2011}. For whatever reason however these systems have fallen out of favour with the public and research communities. It might be that the external viewpoint perspective, or the clunky internal viewpoint are a barrier to communication. \par
In contrast 'immersive' collaborative virtual environments place the user physically into the virtual scene. These systems are less scalable than online virtual communities which can take better advantage of distributed resources \cite{Grimstead2005a, Benford1998}. Goebbels et al. designed a taxonomy for what they termed simply CVE's. In their design they provide an excellent high level description of the ICVE as spaces which provide ``distributed collaborative teams with a virtual space where they could meet as if face-to-face, co-exist and collaborate while sharing and manipulating virtual data", crucially designed in a way to 'disburden' the users' senses \cite{Goebbels2003} and reduce fragmentation of the shared perceptual environment \cite{Roberts2005}. The ability to collaborate in such systems extends even to closely coupled physical tasks \cite{Roberts2012}.\par
There are various technology systems which demonstrate heightened immersion and presence in a virtual environment while allowing interaction between parties who may or may not be physically co-located \cite{Murray2009a}. Attempts at bringing people together in VR extend back to the early days of the technology and systems such as DIVE \cite{Benford:1995hh}. \par
                        Bailenson et al. noted that while increased realism of such avatars increases co-presence it decreases self disclosure \cite{Bailenson2006}. Such systems seem to compromise social interaction even as the realism increases. Avatar representations increased in fidelity and eventually it became possible to share avatar representations of participants wearing body tracking \cite{Schreer2005} and eye tracking equipment \cite{Garau2001, Garau2003}. This enabled tracked, viewpoint independent interaction with reconnection of eye gaze cues within VR in a spatially faithful way \cite{Roberts2003, Murray2007, Steptoe2008, Steptoe2009, Steptoe2010b}.\par
       It is also possible (at least in research implementations) to fully reconstruct people as 3D models, and connect more than two users together utilising life-size multi-view video \cite{Roberts2015, Grau2007, Divorra2010, Gross2003, Beck2013}. These 3D video representations of users can share a virtual space in which spatial and directional information are maintained in a natural way \cite{Roberts2015, Wolff2008}. \par
       Fairchild et al. have developed a system for ICVE and desktop based on earlier work by Duckworth et al. \cite{Duckworth:2012tx}, adapted for use in the Telethrone system \cite{Fairchild2016}.
            \textbf{Eye tracking in ICVE}
                More recently, advanced eye tracking technologies allow the recording and replaying of accurate eye gaze information\cite{Ciger2004a, Steptoe2009, Steptoe2008} alongside information about pupil dilation toward determination of honesty and social presence \cite{Steptoe2010a}.
            Heldal found that collaborative tasks manifested fewer disturbances due to ``misunderstandings of reference or action" when using more immersive systems \cite{Heldal2005}.
        \subsubsection{Situated Displays}
            Between the complexity of ICVEs and the more ubiquitous screen based VC technology there now exist so-called situated displays.\par
    Conversation does not exist in isolation, but rather in the context of semiotic resources. Participants make constant reference to the surrounding assets through mutual orientation, gesture, and diverse cues \cite{Goodwin2000a}.
            So called situated displays seek to embed the represented participant within the spatial and contextual framework of the conversation such that the referential cues are better supported. This has many implications, but chief amongst these is support for a spatially faithful conversational environment supportive of gaze \cite{Pan2014a, Oyekoya2012, Pan2012a, Zhang2013a}.\par
                    Such displays place a representation of the remote user into a space, theoretically allowing all participants to physically interact with the `contextual configurations' around them \cite{Goodwin2000a}. This is a relatively new field of research.  These could include the aforementioned telepresence robots \cite{Lee2011b, Sakamoto2007, Tsui2011}, head in a jar implementations such as SphereAvatar \cite{Oyekoya2012, pan2014comparing, Pan2012a},  and Gaze Preserving Situated Multi-View Telepresence System \cite{Pan2014a}.   Sphereavatar \cite{Oyekoya2012} demonstrates that there are problems with accurate mapping, distortion, and projection, and movement of the captured actors outside of very tight bounds.\par
                     Telehuman brings the whole body of a standing remote user into a space via a cylindrical display with a single tracked observer viewpoint \cite{Kim2012}.\par
            \subsubsection{Retro-reflective Projective Technology (RPT)}
Retro-reflective materials such as Chromatte(tm) cloth reflect light back along the angle of incidence. An everyday application of such material is high visibility jackets.\par
Inami et al. described the first use of RPT in 2000 with their visuo-haptic display \cite{inami2000visuo}. This system used head mounted projectors to augment RPT shapes in the real world. Krum et al. describe the REFLCT system \cite{Krum2012} which uses large retro-reflective surfaces to ``provide[s] users with a personal, perspective correct view of virtual elements that can be used to present social interactions with virtual humans''. They use helmet mounted projectors and describe a military training application in a large volume which allows faithful transmission of attention and gestures from the virtual to the real. They also briefly describe augmenting a facial mannequin by projecting onto RPT adhered to the surface. They point out that the optical characteristics of the material maintain polarisation and so could support passive stereoscopy.\par
Tachi describes an augmented reality system where a helmet mounted projector  places an image of a remote human onto a robot. This is a system they describe as Teleexistance RPT \cite{Tachi2003, Tachi2004}. This system demonstrates a single user, wearing a head mounted projector, viewing only a head which is captured from a single viewpoint. It is however enabled for motion by means of robotics under the Chromatte cloth like the hybrid robotic systems described earlier.\par
Hua et al. demonstrated elements of 'SCAPE', a tracked head mounted projection system surrounded by RPT cloth which could surround multiple physical users in a shared immersive experience \cite{Hua2004}. Hypothetically this system could be extended to include what they term 'passive remote users' projected into the views of the co-located and immersed users. The set-up however is quite complex, and involves wearing headgear, so is again less suitable for informal meetings.\par
Surman et al. discuss a potential development of their retroreflection based auto-stereoscopic display (which head tracks a single user to present a depth enabled stereo pair). They suggest that multiple users could address a large screen, but think that their stereopsis would break down and the system would be extremely challenging \cite{JSID:JSID295}.\par
Although less pertinent to this research there are other interesting deployments using RPT. Darken et al. and later Hahn in his PhD thesis in the same group describe a novel system which mounts a chromakey LED light ring and camera on a HMD \cite{Darken2003}. The camera takes in a mocked up physical cockpit and windows which are made of Chromatte cloth. This video image is very easy to segment to replace the green-screen windows with a simulated external view. The pilot trainee sees their own hands and the cockpit instrument panels (albeit in monoscopic video), alongside the generated external view. This augmented reality is much cheaper to deploy than a full simulator set-up.\par
\subsubsection{Furniture as a Mediating Display}
        Paul Sermon experimented with projection onto beds in his telematic dreaming work \cite{Sermon2000}.\par
Room2room from Microsoft Labs \cite{Pejsa2016} demonstrates the utility of projecting onto furniture by building upon their previous automatic projection mapping research \cite{Jones2014} using their Kinect system and projectors. This system is seen in Figure \ref{fig:room2room}. Their system provides spatially correct viewing through reconstruction but only on a point to point basis with a single user at each end. 

    \subsubsection{Augmented Reality Collaboration}
    Lehment at al described what they term a ``consensus reality" for head mounted AR in which they compute the correct locations for remote participants as 3D video representations overlaid on the real \cite{Lehment2014}.

              \subsection{Network Issues}
            Bradner \& Mark established that latency matters in remote telecollaboration \cite{Bradner:2002:WDM:587078.587110}, while an extensive body of work notes that system latency impacts key factors such as interruptability \cite{Avrahami2007}. Gergle et al. found that remote collaborating pairs of people were tolerant of delays in visual feedback up to 939ms, after which shared task performance suffered considerably \cite{Gergle2006}. Overall it is clear that for communication systems, especially those with potential for shared or collaborative task, those aspects of the system latency which can be controlled and/or minimised should be.\par
             Early research around COVEN and DIVE described the benefits of multicast for multiparty collaborative technology systems. While multicast still has advantages in latency critical set-ups the bandwidth/throughput has to a degree `caught up', while the technical demands of a wide area multicast network remain the same. The DIVE system has a standalone WAN application layer network transport called DIVEBONE which attempted to address this issue \cite{Greenhalgh2001a}. Steed et al. describe a hybrid multi/uni cast system which allows local multicast with unicast on the WAN \cite{Greenhalgh2001}.\par
              Bradner used social impact theory to suggest  it is important to either say the distally located person is geographically remote to emulate real systems, or else explain they are in the next room to exclude this factor \cite{Bradner:2002:WDM:587078.587110}, while an extensive body of work notes that system latency impacts key factors such as interruptibility \cite{Avrahami2007}.\par
         Gasparello et al. \cite{Gasparello:2011in} then later Moore et al. \cite{Moore:2010jt} worked within the wider research group on novel network transport systems for synchronised 2D and 3D video information to support telepresence reconstruction in the later withyou system \cite{Roberts2015}. \par
       When Lamboray extended their blue-c 3D video system they considered that latency of up to 200ms was acceptable so long as jitter (changes in latency) remained low \cite{Park1999, lamboray2005data}.

        \section{Support for Less Formal?}
        
Outside of the sphere of simple webcam systems such as Zoom and their ilk there has been little attention invested in advanced telecommunication tools for informal settings. This is exactly the kind of thing necessary to enable home working in the future.\par
Slovak et al. describe a group-to-group videoconferencing system called GColl in which they highlight mutual gaze support and modest technical requirements \cite{Slovak2009}. GColl is a monitor based system which seeks to align mutual gaze by rendering to a small window within 5 degreesof the mounted camera. It seems to address the right problems but is a questionable development over and above a careful videoconferencing set-up. De Greef and Ijsselsteijn describe a system which provides collaborative working tools alongside video and audio conferencing for the home \cite{DeGreef2001}. Judge and Neustaedter examined how 21 families used videoconferencing in a home setting and found that requirement for planning and availability for using the tools inhibited their use. They suggest an 'always on' system might better mediate availability \cite{Judge2010}. \par
Interestingly Lottridge et al. identified that it is the empty reflective moments during mundane activity which might benefit most from the ability to strike up intimate conversations between separated couples \cite{Lottridge2009}. Whether and how much this supposition is analogous to the so-called `water cooler meeting' moments in business remains an open question but is perhaps worth bearing in mind.\par
There is, of course, Room2Room which provides a far better informal home experience, but is inherently one to one \cite{Pejsa2016}. The same is true of 'Holoportation', a reconstructed and tracked 3D video based system designed by Microsoft for head mounted display and seemingly aimed at the home market \cite{orts2016holoportation}. Similarly utilising Microsoft hardware, but on a tight budget, HomeProxy uses a simple combination of consumer devices to render a reconstructed remote partner to a cabinet style display on a desk \cite{Tang2013}. Users reported that they appreciated the look and feel of the HomeProxy system which is designed to fit in sympathetically to home furnishings.\par
Gaver explains that ''the “space” created by audio-video technologies is significantly different from spaces as found in hallways, offices or meeting rooms", conveying a limited subset of visual and auditory information, preventing movement and exploration, and is often arbitrary and discontinuous. He says that ``these properties shape the possibilities for collaboration." \cite{Gaver1992}.\par
In a work setting Fish et al. said (in 1993) that ``informal communication cuts across organisational boundaries and often happens spontaneously." They reference research which suggests that informal communication is more frequent, expressive, and interactive \cite{Fish1993}. Fish went on to look at early telepresence systems which might support informal, but by any standard these systems are too clunky to be deemed capable of such. Although this has been recognised as an issue for decades it could nonetheless seem that less formal business meetings are of insufficient utility to attract the technology and research to better support them. Computer Supported Collaborative Working (CSCW) is a well known acronym  but there seems to be no single point of entry to research into less formal systems. This could be a function of the technology being better suited to deployment in a controlled formal setting. \par
The paper ``Ad hoc versus established groups in an electronic meeting system environment" turns out to be using the other definition of ad-hoc meetings which is meetings convened from random members with no past and no future together. The study is also dated being from 1990, but still provides insight. In the paper Dennis et al. reviewed what little research existed at the time noted that meeting outcomes varied between studies as to whether ad-hoc or more established and formal meetings were more productive \cite{Dennis1990}.\par
%Reconnection of naturalistic non-verbal cues bolsters turn passing \cite{Gu2006}, trust, empathy, and rapport between co-located, and telepresent users \cite{Bohannon2013}. The Telethrone aims to address this requirement for a simple, deployable, pervasive, group telecommunication system with spatial, non-verbal cue support. 
            Fayard and Weeks examined the affordances of informal interaction (indeed exactly the `water cooler meetings' that the BBC were talking about) \cite{Fayard2007}. They identified: proximity, privacy, and legitimacy as key affordances which should be supported, and highlighted: functional centrality, semi-enclosure, reciprocal visibility, easy access and egress, multiple shared resources as important further affordances of the environment. The affordance of legitimacy is interesting. This means that the location of the informal meeting should support the feeling that it is legitimate to be there and strike up a conversation. 
            A potential model example is explored in Horgan's book `Excellence by design: Transforming workplace and work practice'. `The LX common' was a semi communal space for informal meetings. It was semi-enclosed to give some privacy, but located on a busy thoroughfare and housed the photocopiers, printers, shared reference material, kitchen etc.  People who passed felt free to listen and occasionally join in. Three rules evolved from the research: Traffic through the common was acceptable at any time, anyone was free to join any meeting in the common, anyone was free to leave any meeting in the common at any time \cite{horgen1999excellence}.\par
            Supporting spatial aspects such as mutual gaze or `full gaze awareness'' \cite{Monk2002} in this way normally demands large purpose built installations which poorly support casual or ad hoc meeting paradigms \cite{Schroeder2001, Wolff2008}. It seems clear that there is a justification for development of simple affordable technologies which better mediates communication over distance for less formal situations.           


\subsection{Support for Dynamic Meetings}
Voida et al. discuss natural sub grouping in partially distributed teams \cite{Voida2012}. These `fault lines' within meetings are a natural component of more informal meeting structure with groups potentially forming and dispersing naturally over time. \par           
Greenburg and Jerald found in group interaction with trainers that members often changed their seating position to facilitate discussion goals \cite{greenberg1976role}. This may not be true of all group meetings but it is under supported in telepresence systems.\par
            This dynamic reorganisation of meetings is potentially an affordance for ad-hoc and casual meeting styles. It demands the ability to reorganise chairs such that they still have faithful views onto remote participants. There is no support for this found in literature.\par


\section{Summary}
Attention is mediated in a large part by non-verbal cues. A large body of research across psychology tells us that Extra Verbal Content is important \cite{Goodwin2000a} for mediating attention \cite{Stiefelhagen2001, Sellen1992, Stiefelhagen2002, Deckers2013, Gibson1963, Bente1998, Slessor2008, Argyle, Hofmann20061683, Pan2008a, Vertegaal1997}, conversational flow, and transmission of mood and manner.\par
Support for these cues is limited in conventional commercial systems which convey the image of a person over a distance through technology. \par
Eye gaze, mutual eye gaze, and facial expression \cite{Hager1979, Argyle, ohba1998facial} are particularly rich information channels, as are body torque, gesture, and head gaze. Proxemics shows us that interpersonal distance is an important feature of communication and can evolve as situations change \cite{Hall1969}.\par
It is especially difficult to support mutual eye gaze well because of the Mona Lisa effect where a viewer of a screen showing a camera view of a remote participant will only perceive that they are being looked at when the remote user is looking directly into the camera lens which captures them. Fixing this disconnect of mutual gaze in telepresence systems is an active area in both research and commercial system. \par
Telepresence solutions aimed at collaborative working have traditionally attempted to join remote spaces so that people in each can look into the other, seeing each other, at best, as if through a window. Immersive Collaborative Virtual Environments (ICVE) have been used to join remote spaces so that they coincide. Free-viewpoint video has been combined with immersive projection technology to allow people to move around spaces, seemingly together \cite{Roberts2015}. For these spaces to be aligned so that they coincide, removing the impression of being separated by glass, users must wear stereo glasses. \par
More recently, situated displays have attempted to give the impression of placing people within each other's room, but either lack the spatial faithfulness available within ICVE, or else do not render the whole of the remote collaborator.  \par
Situated displays can place a representation of the remote user within a space. Most informative to the research at the time of the literature review was the work of Pan et al. \cite{Oyekoya2012, pan2014comparing, Pan2012a, Pan2014a}. Paul Sermon explored situating live telepresence video onto furniture \cite{Sermon2000}.  \par
The bulk of commodity systems for telecommunication remain dyadic, that is, person to person, though this is changing fast in the face of the 2020 global pandemic. More interesting and difficult is small group interaction, so-called `teleconferencing'. This is a difficult field, with known issues in all of the market leading systems. Mona Lisa effects, latency, jitter, and the requirement of a lot of expensive equipment to be maintained alongside a booking system for its use amongst the most problematic.\par
Inami pioneered the use of RPT \cite{inami2000visuo} and this was built on by others, most notably Tachi \cite{Tachi2003, Tachi2004}. Tachi's projection of a face onto a robotically manipulated surface has some similarity to Krum et al's REFLCT system \cite{Krum2012} in that observers move around to gain viewpoint independent and spatially segmented views on a retro-reflective surface, but the application is very different. Retro reflection (light field) is used in the research leading Telehuman 2 \cite{gotsch2018telehuman2}, and the small group Telethrone system \cite{o2018telethrone}. \par

The Holovizio real-time 3D light field transmission system\cite{3dlightfield} is seen in Table \ref{compare}.\par
Maimone et al. presented their ``Encumbrance free telepresence" \cite{6162881}, which was not captured in the initial review but is now added to Table \ref{compare}. It can be seen that this system provides many of the desired affordances but falls short on being able to support small groups. \
During the peer-review process the ``SCAPE" system \cite{1255811} was highlighted. This comparatively complex system combines immersive projection from head mounted projectors (onto surrounding walls and objects) with tracking and virtual avatars to create multiple viewpoints into a shared virtuality.


        \subsection{Comparing Key Systems and their Affordances}
Affordances are properties of the environment that offer actions to appropriate organisms \cite{gibson2014ecological}. In this research the Gibson definition of `an action possibility' is adopted rather than the more restrictive Norman definition of `a perceived suggestion' \cite{norman2002psychopathology}. McGrenere and Ho sum up the difference in definitions of affordances by considering a hidden door in a paneled room \cite{McGrenere2000}. In Gibson's definition the existence of the door (or parallax cues, or stereoscopy, or eye gaze for instance in this research) does not need to be explicitly known to the user, just its existence is enough. In Norman's view the door would need to have a handle to suggest the action to the user. This is a less relevant HCI definition for this research.\par
%Triple-view is an important system \cite{Canny2011}. It affords correct spatial alignment through the use of an interpolated camera pair per screen, providing three bi-directional channels which preserve the directionality of the user's gaze. Importantly this system still uses screens which show backgrounds from the remote space resulting in the feeling of looking into another space. \par
%The more technically demanding Immersive Group-to-Group \cite{Beck2013} wall places the reconstructed remote users in a bland virtual space which can be set to match the surrounding walls and draws less attention to the elements which are not the remote collaborator.\par
%The previously mentioned TeleHuman supports 360 degrees motion parallax as the viewer moves around a human scale cylinder which is internally projected. This is optionally stereoscopic 3D \cite{Kim2012}.\par
%          There are historical research system which address this requirement in simple video. In 2006 the i2i system interpolated between multiple camera views from around a small monitor and claimed to increase trust by reuniting mutual eye gaze \cite{Bekkering2006}. The GAZE groupware system \cite{Vertegaal1999} utilised eye tracking to switch video channels in a virtual room. 

\section{Theoretical Framework}
        
\subsection{Problem Statement}

%\subsection{Exploring Research Questions}        
        
            It is clear that there are multiple factors which contribute to successful human-human communication. These factors remain important in telecommunication supported by technology, and are variously supported, unsupported, or modified by particular technologies.\par
            Of particular importance is interpersonal gaze \cite{Cook1977, Kleinke1986a, Fagel2010}, and gaze is an excellent dependant variable for experimentation. Non-verbal cues are also important across multiple modalities of sight, sound \cite{Otsuka2005}, and position of interlocutors \cite{Kendon1967}, extending to the whole body \cite{Kleinke1986a, Nguyen2009}.\par
            The Mona Lisa effect demands that some novel system which allows spatial signals to be maintained must be employed \cite{Moubayed2012}.\par
            While formal meeting paradigms are supported to an extent by commercially deployed systems this does not suit all meeting styles. Such systems are generally booked well in advance and so meetings tend toward a formal structure. These meetings seem to demand many smaller supporting meetings between parties or groups of parties.\par
The `problem' is a supporting technology for small less formal groups. One which connects home and work spaces without bringing in those backgrouds, creating a level playing field. A fully pervasive system could also allow dynamism and movement, connection of natural non vocal cues, without too much encumbering technology overhead.
            
            \subsection{Core Assumptions}
                Figure \ref{fig:Framework} shows the interlocking relationships between baseline communication where the participants are present, and technology which attempts to support across distance.
                \begin{figure*}[h]
                \centering
                \includegraphics[width=\textwidth]{frameworkVenn.png}
                \caption{The Venn diagram shows areas of research which have been identified in blue. These interlock and overlap as shown. The most relevant identified researchers from the literature are shown in black close to the fields of study which they represent. This diagram is a view of the core assumptions for the research, with the most important fields at the centre. }
                \label{fig:Framework}
                \end{figure*}
                
Of most interest to this research is the centre of the Venn where meeting styles which are less formal, and perhaps dynamic, may occur. Looking at these items one by one gives us our core assumptions.

                \begin{enumerate}
                \item Gaze\par
                 Gaze is broadly agreed to be highly important for mediating flow. Mutual gaze is a rich emotional channel. The research must consider gaze. All of the researchers listed around the Venn have at some point engaged with this topic.
                \item Attention\par
                The non-verbal communication channel employed in `attention' is assumed based upon the literature to be critical to smoothly leaving and entering a fast flowing conversation where concentration around a defined problem may be high (gesturing to a chair for instance). Again, all of the listed researchers have made reference to attention in their work.
                \item Spatial (immersive)\par
Support for spatiality is important in a group setting so that directional non-verbal cues can find their target. The topic of spatial relationships between interlocutors cuts across all of the researchers, but this is not true of immersion. Immersion in a shared virtuality can certainly support the underlying requirements spatial, but the technical infrastructure required is out of scope (so this is struck through on the diagram). Roberts and Steed are the main expertise referenced even though this element is not expanded in the research.
                \item Situated\par
                Situated displays are those which are appropriate for their surrounding context, in this case the informal meeting. Roberts, Pan, Steed and Steptoe seem the most relevant researchers in these technology spaces.
                \item Informal\par
                   Based on the literature proxemics is believed to be relevant in a meeting where subgroups can be instantiated and destroyed as the meeting evolves, and those where people can be invited in from outside the physical bounds of the meeting (informal spaces). Hall is the best source for this work. If it is assumed that people may come and go, and subgroups may be convened then Sermon and Benford are the best references through their work blending real and virtual spaces. This may be more consistent with less organised meetings such as those convened on demand (ad-hoc).
                \end{enumerate}

            \subsection{Peripheral Assumptions}
                Surrounding the centre of the Venn are additional relevant topics from social science branches of theory\par
                \textbf{From verbal communication}\par
                It is assumed that the directionality of sound is important \cite{Aoki2003}, and this will be engineered into the experimental design. It is assumed that movement of the lips is an indicator and this is tied to latency and frame rate in the vision system.\par
                \textbf{From non-verbal communication}\par
                It is assumed that eye gaze is of high importance, and that this information channel is supported by head gaze and body torque to a high degree. It is further assumed that mutual eye gaze is of less relevance in a multi party meeting where there is a common focus for attention but can be significant for turn passing.
                It is assumed that upper body framing and support for transmission of micro and macro gesturing is important for signaling attention in the broader group, and for message passing in subgroups.
                
                        \begin{figure}[h!] \includegraphics[width=\linewidth]{conceptualFrameworkBoxandStick.png} \caption{Connected concepts revealed during the review, and roughly how they relate to one another. The web moves top left to bottom right as the attention focuses. The left hand column shows the broad flow from interpersonal communication in the real (top) to home working (bottom).} \label{fig:conceptualFramework} \end{figure}
